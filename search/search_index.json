{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to fsspec-utils!","text":"<p><code>fsspec-utils</code> is a powerful library designed to enhance <code>fsspec</code> (Filesystem Spec) with advanced utilities and extensions, making multi-format I/O, cloud storage configuration, caching, monitoring, and batch processing more streamlined and efficient.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>This library aims to simplify complex data operations across various file systems, providing a unified and extended interface for handling diverse data formats and storage solutions. Whether you're working with local files, cloud storage like AWS S3, Azure Blob Storage, or Google Cloud Storage, <code>fsspec-utils</code> provides the tools to manage your data effectively.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Multi-format Data I/O: Seamlessly read and write data in various formats, including JSON, CSV, and Parquet.</li> <li>Cloud Storage Configuration: Simplified utilities for configuring and interacting with different cloud storage providers.</li> <li>Enhanced Caching and Monitoring: Improve performance and gain insights into your data operations with built-in caching mechanisms and monitoring capabilities.</li> <li>Batch Processing and Parallel Operations: Efficiently handle large datasets and execute operations in parallel for improved throughput.</li> <li>Directory-like Filesystem: Interact with nested data structures as if they were traditional directories, even on object stores.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Check out our Quickstart Guide to begin using <code>fsspec-utils</code> in your projects.</p>"},{"location":"#badges","title":"Badges","text":""},{"location":"advanced/","title":"Advanced Usage","text":"<p><code>fsspec-utils</code> extends the capabilities of <code>fsspec</code> to provide a more robust and feature-rich experience for handling diverse file systems and data formats. This section delves into advanced features, configurations, and performance tips to help you get the most out of the library.</p>"},{"location":"advanced/#unified-filesystem-creation-with-filesystem","title":"Unified Filesystem Creation with <code>filesystem</code>","text":"<p>The <code>fsspec_utils.core.filesystem</code> function offers a centralized and enhanced way to instantiate <code>fsspec</code> filesystem objects. It supports:</p> <ul> <li>Intelligent Caching: Automatically wraps filesystems with <code>MonitoredSimpleCacheFileSystem</code> for improved performance and verbose logging of cache operations.</li> <li>Structured Storage Options: Integrates seamlessly with <code>fsspec_utils.storage_options</code> classes, allowing for type-safe and organized configuration of cloud and Git-based storage.</li> <li>Protocol Inference: Can infer the filesystem protocol directly from a URI or path, reducing boilerplate.</li> </ul> <p>Example: Cached S3 Filesystem with Structured Options</p> <pre><code>from fsspec_utils.core import filesystem\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Configure S3 options using the structured class\ns3_opts = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create a cached S3 filesystem using the 'filesystem' helper\nfs = filesystem(\n    \"s3\",\n    storage_options=s3_opts,\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\", # Optional: specify cache directory\n    verbose=True # Enable verbose cache logging\n)\n\n# Use the filesystem as usual\nprint(fs.ls(\"s3://your-bucket/\"))\n</code></pre>"},{"location":"advanced/#detailed-caching-for-improved-performance","title":"Detailed Caching for Improved Performance","text":"<p><code>fsspec-utils</code> provides an enhanced caching mechanism that improves performance for repeated file operations, especially useful for remote filesystems.</p> <p>This example demonstrates how caching improves read performance. The first read populates the cache, while subsequent reads retrieve data directly from the cache, significantly reducing access time. It also shows that data can still be retrieved from the cache even if the original source becomes unavailable.</p> <p>Caching in fsspec-utils is an enhanced mechanism that improves performance for repeated file operations, especially useful for remote filesystems where network latency can significantly impact performance.</p> <p>The <code>filesystem()</code> function provides several parameters for configuring caching:</p> <ul> <li><code>cached</code>: When set to <code>True</code>, enables caching for all read operations</li> <li><code>cache_storage</code>: Specifies the directory where cached files will be stored</li> <li><code>verbose</code>: When set to <code>True</code>, provides detailed logging about cache operations</li> </ul> <p>Step-by-step walkthrough:</p> <ol> <li> <p>First read (populating cache): When reading a file for the first time, the data is retrieved from the source (disk, network, etc.) and stored in the cache directory. This takes longer than subsequent reads because it involves both reading from the source and writing to the cache.</p> </li> <li> <p>Second read (using cache): When the same file is read again, the data is retrieved directly from the cache instead of the source. This is significantly faster because it avoids network latency or disk I/O.</p> </li> <li> <p>Demonstrating cache effectiveness: Even after the original file is removed, the cached version can still be accessed. This demonstrates that the cache acts as a persistent copy of the data, independent of the source file.</p> </li> <li> <p>Performance comparison: The timing results clearly show the performance benefits of caching, with subsequent reads being orders of magnitude faster than the initial read.</p> </li> </ol> <p>This caching mechanism is particularly valuable when working with:</p> <ul> <li>Remote filesystems (S3, GCS, Azure) where network latency is a bottleneck</li> <li>Frequently accessed files that don't change often</li> <li>Applications that read the same data multiple times</li> <li>Environments with unreliable network connections</li> </ul>"},{"location":"advanced/#setup-and-first-read-populating-cache","title":"Setup and First Read (Populating Cache)","text":"<p>In this step, we create a sample JSON file and initialize the <code>fsspec-utils</code> filesystem with caching enabled. The first read operation retrieves data from the source and populates the cache.</p> <p>Setup steps:</p> <ol> <li>Create a temporary directory for our example</li> <li>Create sample data file</li> <li>Configure filesystem with caching</li> </ol> <pre><code>import tempfile\nimport time\nimport os\nimport shutil\nfrom fsspec_utils import filesystem\nfrom examples.caching.setup_data import create_sample_data_file\n\ntmpdir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {tmpdir}\")\n\nsample_file = create_sample_data_file(tmpdir)\n\ncache_dir = os.path.join(tmpdir, \"cache\")\nfs = filesystem(\n    protocol_or_path=\"file\",\n    cached=True,\n    cache_storage=cache_dir,\n    verbose=True\n)\n\nprint(\"\\n=== First read (populating cache) ===\")\nstart_time = time.time()\ndata1 = fs.read_json(sample_file)\nfirst_read_time = time.time() - start_time\nprint(f\"First read completed in {first_read_time:.4f} seconds\")\n</code></pre>"},{"location":"advanced/#second-read-using-cache","title":"Second Read (Using Cache)","text":"<p>Now, let's read the same file again to see the performance improvement from using the cache.</p> <pre><code>print(\"\\n=== Second read (using cache) ===\")\nstart_time = time.time()\ndata2 = fs.read_json(sample_file)\nsecond_read_time = time.time() - start_time\nprint(f\"Second read completed in {second_read_time:.4f} seconds\")\n</code></pre> <p>The second read retrieves data directly from the cache, which is significantly faster than reading from the source again.</p>"},{"location":"advanced/#reading-from-cache-after-deletion","title":"Reading from Cache after Deletion","text":"<p>To demonstrate that the cache is persistent, we'll remove the original file and try to read it again.</p> <pre><code>print(\"\\n=== Demonstrating cache effectiveness ===\")\nprint(\"Removing original file...\")\nos.remove(sample_file)\nprint(f\"Original file exists: {os.path.exists(sample_file)}\")\n\nprint(\"\\n=== Third read (from cache only) ===\")\nstart_time = time.time()\ndata3 = fs.read_json(sample_file)\nthird_read_time = time.time() - start_time\nprint(f\"Third read completed in {third_read_time:.4f} seconds\")\nprint(\"\u2713 Successfully read from cache even after original file was removed\")\n\nprint(\"\\n=== Performance Comparison ===\")\nprint(f\"First read (from disk): {first_read_time:.4f} seconds\")\nprint(f\"Second read (from cache): {second_read_time:.4f} seconds\")\nprint(f\"Third read (from cache): {third_read_time:.4f} seconds\")\n\nshutil.rmtree(tmpdir)\nprint(f\"Cleaned up temporary directory: {tmpdir}\")\n</code></pre> <p>This step proves that the cache acts as a persistent copy of the data, allowing access even if the original source is unavailable.</p>"},{"location":"advanced/#custom-filesystem-implementations","title":"Custom Filesystem Implementations","text":"<p><code>fsspec-utils</code> provides specialized filesystem implementations for unique use cases:</p>"},{"location":"advanced/#gitlab-filesystem-gitlabfilesystem","title":"GitLab Filesystem (<code>GitLabFileSystem</code>)","text":"<p>Access files directly from GitLab repositories. This is particularly useful for configuration files, datasets, or code stored in private or public GitLab instances.</p> <p>Example: Reading from a GitLab Repository</p> <pre><code>from fsspec_utils.core import filesystem\n\n# Instantiate a GitLab filesystem\ngitlab_fs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"your-group/your-project\", # Or \"project_id\": 12345\n        \"ref\": \"main\", # Branch, tag, or commit SHA\n        \"token\": \"glpat_YOUR_PRIVATE_TOKEN\" # Required for private repos\n    }\n)\n\n# List files in the repository root\nprint(gitlab_fs.ls(\"/\"))\n\n# Read a specific file\ncontent = gitlab_fs.cat(\"README.md\").decode(\"utf-8\")\nprint(content[:200]) # Print first 200 characters\n</code></pre>"},{"location":"advanced/#advanced-data-reading-and-writing-read_files-write_files","title":"Advanced Data Reading and Writing (<code>read_files</code>, <code>write_files</code>)","text":"<p>The <code>fsspec_utils.core.ext</code> module (exposed via <code>AbstractFileSystem</code> extensions) provides powerful functions for reading and writing various data formats (JSON, CSV, Parquet) with advanced features like:</p> <ul> <li>Batch Processing: Efficiently handle large datasets by processing files in configurable batches.</li> <li>Parallel Processing: Leverage multi-threading to speed up file I/O operations.</li> <li>Schema Unification &amp; Optimization: Automatically unifies schemas when concatenating multiple files and optimizes data types for memory efficiency (e.g., using Polars' <code>opt_dtypes</code> or PyArrow's schema casting).</li> <li>File Path Tracking: Optionally include the source file path as a column in the resulting DataFrame/Table.</li> </ul>"},{"location":"advanced/#universal-read_files","title":"Universal <code>read_files</code>","text":"<p>The <code>read_files</code> function acts as a universal reader, delegating to format-specific readers (JSON, CSV, Parquet) while maintaining consistent options.</p> <p>Example: Reading CSVs in Batches with Parallelism</p> <pre><code>from fsspec_utils.core import filesystem\n\n# Assuming you have multiple CSV files like 'data/part_0.csv', 'data/part_1.csv', etc.\n# on your local filesystem\nfs = filesystem(\"file\")\n\n# Read CSV files in batches of 10, using multiple threads, and including file path\nfor batch_df in fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    batch_size=10,\n    include_file_path=True,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processed batch with {len(batch_df)} rows. Columns: {batch_df.columns}\")\n    print(batch_df.head(2))\n</code></pre>"},{"location":"advanced/#reading-and-processing-multiple-files-pyarrow-tables-batch-processing","title":"Reading and Processing Multiple Files (PyArrow Tables, Batch Processing)","text":"<p><code>fsspec-utils</code> simplifies reading multiple files of various formats (Parquet, CSV, JSON) from a folder into a single PyArrow Table or Polars DataFrame.</p> <p>Reading multiple files into a single table is a powerful feature that allows you to efficiently process data distributed across multiple files. This is particularly useful when dealing with large datasets that are split into smaller files for better organization or parallel processing.</p> <p>Key concepts demonstrated:</p> <ol> <li> <p>Glob patterns: The <code>**/*.parquet</code>, <code>**/*.csv</code>, and <code>**/*.json</code> patterns are used to select files recursively from the directory and its subdirectories. The <code>**</code> pattern matches any directories, allowing the function to find files in nested directories.</p> </li> <li> <p>Concat parameter: The <code>concat=True</code> parameter tells the function to combine data from multiple files into a single table or DataFrame. When set to <code>False</code>, the function would return a list of individual tables/DataFrames.</p> </li> <li> <p>Format flexibility: The same interface can be used to read different file formats (Parquet, CSV, JSON), making it easy to work with heterogeneous data sources.</p> </li> </ol> <p>Step-by-step explanation:</p> <ol> <li> <p>Creating sample data: We create two subdirectories and populate them with sample data in three different formats (Parquet, CSV, JSON). Each format contains the same structured data but in different serialization formats.</p> </li> <li> <p>Reading Parquet files: Using <code>fs.read_parquet(\"**/*.parquet\", concat=True)</code>, we read all Parquet files recursively and combine them into a single PyArrow Table. Parquet is a columnar storage format that is highly efficient for analytical workloads.</p> </li> <li> <p>Reading CSV files: Using <code>fs.read_csv(\"**/*.csv\", concat=True)</code>, we read all CSV files and combine them into a Polars DataFrame, which we then convert to a PyArrow Table for consistency.</p> </li> <li> <p>Reading JSON files: Using <code>fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)</code>, we read all JSON files and combine them into a Polars DataFrame, then convert it to a PyArrow Table.</p> </li> <li> <p>Verification: Finally, we verify that all three tables have the same number of rows, confirming that the data was correctly read and combined across all files and formats.</p> </li> </ol> <p>The flexibility of <code>fsspec-utils</code> allows you to use the same approach with different data sources, including remote filesystems like S3, GCS, or Azure Blob Storage, simply by changing the filesystem path.</p>"},{"location":"advanced/#setup","title":"Setup","text":"<p>First, we'll create a temporary directory with sample data in different formats.</p> <p>Setup steps:</p> <ol> <li>Create a temporary directory for our example</li> <li>Create sample data in subdirectories</li> </ol> <pre><code>import tempfile\nimport shutil\nimport os\nfrom examples.read_folder.create_dataset import create_sample_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_sample_dataset(temp_dir)\n</code></pre> <p>This step sets up the environment by creating a temporary directory and populating it with sample data files.</p>"},{"location":"advanced/#reading-parquet-files","title":"Reading Parquet Files","text":"<p>Now, let's read all the Parquet files from the directory and its subdirectories into a single PyArrow Table.</p> <p>Reading Parquet files:</p> <ol> <li>Read Parquet files using glob pattern</li> <li>Display table information and sample data</li> </ol> <pre><code>print(\"\\n=== Reading Parquet Files ===\")\nfrom fsspec_utils import filesystem\nfs = filesystem(temp_dir)\nparquet_table = fs.read_parquet(\"**/*.parquet\", concat=True)\nprint(f\"Successfully read Parquet files into PyArrow Table\")\nprint(f\"Table shape: {parquet_table.num_rows} rows x {parquet_table.num_columns} columns\")\nprint(\"First 3 rows:\")\nprint(parquet_table.slice(0, 3).to_pandas())\n</code></pre> <p>We use the <code>read_parquet</code> method with a glob pattern <code>**/*.parquet</code> to find all Parquet files recursively. The <code>concat=True</code> parameter combines them into a single table.</p>"},{"location":"advanced/#reading-csv-files","title":"Reading CSV Files","text":"<p>Next, we'll read all the CSV files into a Polars DataFrame and then convert it to a PyArrow Table.</p> <p>Reading CSV files:</p> <ol> <li>Read CSV files using glob pattern</li> <li>Display DataFrame information and sample data</li> <li>Convert to PyArrow Table for consistency</li> </ol> <pre><code>print(\"\\n=== Reading CSV Files ===\")\ncsv_df = fs.read_csv(\"**/*.csv\", concat=True)\nprint(f\"Successfully read CSV files into Polars DataFrame\")\nprint(f\"DataFrame shape: {csv_df.shape}\")\nprint(\"First 3 rows:\")\nprint(csv_df.head(3))\ncsv_table = csv_df.to_arrow()\n</code></pre> <p>Similarly, we use <code>read_csv</code> with the same glob pattern to read all CSV files.</p>"},{"location":"advanced/#reading-json-files","title":"Reading JSON Files","text":"<p>Finally, let's read all the JSON files.</p> <p>Reading JSON files:</p> <ol> <li>Read JSON files using glob pattern</li> <li>Display DataFrame information and sample data</li> <li>Convert to PyArrow Table for consistency</li> </ol> <pre><code>print(\"\\n=== Reading JSON Files ===\")\njson_df = fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)\nprint(f\"Successfully read JSON files into Polars DataFrame\")\nprint(f\"DataFrame shape: {json_df.shape}\")\nprint(\"First 3 rows:\")\nprint(json_df.head(3))\njson_table = json_df.to_arrow()\n</code></pre> <p>The <code>read_json</code> method is used to read all JSON files. We set <code>as_dataframe=True</code> to get a Polars DataFrame.</p>"},{"location":"advanced/#verification","title":"Verification","text":"<p>Let's verify that all the tables have the same number of rows.</p> <pre><code>print(\"\\n=== Verification ===\")\nprint(f\"All tables have the same number of rows: {parquet_table.num_rows == csv_table.num_rows == json_table.num_rows}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\n</code></pre> <p>This final step confirms that our data reading and concatenation were successful.</p> <p>This example shows how to read various file formats from a directory, including subdirectories, into a unified PyArrow Table or Polars DataFrame. It highlights the flexibility of <code>fsspec-utils</code> in handling different data sources and formats.</p> <p><code>fsspec-utils</code> enables efficient batch processing of large datasets by reading files in smaller, manageable chunks. This is particularly useful for memory-constrained environments or when processing streaming data.</p> <p>Batch processing is a technique for handling large datasets by dividing them into smaller, manageable chunks. This is particularly important for:</p> <ol> <li>Memory-constrained environments: When working with datasets that are too large to fit in memory, batch processing allows you to process the data incrementally.</li> <li>Streaming data: When data is continuously generated (e.g., from IoT devices or real-time applications), batch processing enables you to process data as it arrives.</li> <li>Distributed processing: In distributed computing environments, batch processing allows different nodes to work on different chunks of data simultaneously.</li> </ol> <p>The <code>batch_size</code> parameter controls how many files or records are processed together in each batch. A smaller batch size reduces memory usage but may increase processing overhead, while a larger batch size improves throughput but requires more memory.</p> <p>Step-by-step walkthrough:</p> <ol> <li> <p>Creating sample batched data: We generate sample data and distribute it across multiple files in each format (Parquet, CSV, JSON). Each file contains a subset of the total data, simulating a real-world scenario where data is split across multiple files.</p> </li> <li> <p>Reading Parquet files in batches: Using <code>fs.read_parquet(parquet_path, batch_size=2)</code>, we read all Parquet files in batches of 2 files at a time. Each iteration of the loop processes a batch of files, and the <code>batch</code> variable contains the combined data from those files.</p> </li> <li> <p>Reading CSV files in batches: Similarly, we use <code>fs.read_csv(csv_path, batch_size=2)</code> to read CSV files in batches. The result is a Polars DataFrame for each batch, which we can process individually.</p> </li> <li> <p>Reading JSON files in batches: Finally, we use <code>fs.read_json(json_path, batch_size=2)</code> to read JSON files in batches. The JSON data is automatically converted to Polars DataFrames for easy processing. <pre><code>```python\nprint(\"\\n=== JSON Batch Reading ===\")\njson_path = os.path.join(temp_dir, \"*.json\")\nprint(\"\\nReading JSON files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_json(json_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\n</code></pre></p> </li> </ol> <p>The <code>read_json</code> method is also used with <code>batch_size=2</code> to process JSON files in batches.</p> <p>This example illustrates how to read Parquet, CSV, and JSON files in batches using the <code>batch_size</code> parameter. This approach allows for processing of large datasets without loading the entire dataset into memory at once.</p>"},{"location":"advanced/#advanced-parquet-handling-and-delta-lake-integration","title":"Advanced Parquet Handling and Delta Lake Integration","text":"<p><code>fsspec-utils</code> enhances Parquet operations with deep integration with PyArrow and Pydala, enabling efficient dataset management, partitioning, and delta lake capabilities.</p> <ul> <li><code>pyarrow_dataset</code>: Create PyArrow datasets for optimized querying, partitioning, and predicate pushdown.</li> <li><code>pyarrow_parquet_dataset</code>: Specialized for Parquet, handling <code>_metadata</code> files for overall dataset schemas.</li> <li><code>pydala_dataset</code>: Integrates with <code>pydala</code> for advanced features like Delta Lake operations (upserts, schema evolution).</li> </ul> <p>Example: Writing to a PyArrow Dataset with Partitioning</p> <pre><code>import polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\nbase_path = \"output/my_partitioned_data\"\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"value\": [\"A\", \"B\", \"C\", \"D\"],\n    \"year\": [2023, 2023, 2024, 2024],\n    \"month\": [10, 11, 1, 2]\n})\n\n# Write data as a partitioned PyArrow dataset\nfs.write_pyarrow_dataset(\n    data=data,\n    path=base_path,\n    partition_by=[\"year\", \"month\"], # Partition by year and month\n    format=\"parquet\",\n    compression=\"zstd\",\n    mode=\"overwrite\" # Overwrite if path exists\n)\n\nprint(f\"Data written to {base_path} partitioned by year/month.\")\n# Expected structure: output/my_partitioned_data/year=2023/month=10/data-*.parquet\n</code></pre> <p>Example: Delta Lake Operations with Pydala Dataset</p> <pre><code>import polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\ndelta_path = \"output/my_delta_table\"\n\n# Initial data\ninitial_data = pl.DataFrame({\n    \"id\": [1, 2],\n    \"name\": [\"Alice\", \"Bob\"],\n    \"version\": [1, 1]\n})\n\n# Write initial data to a Pydala dataset\nfs.write_pydala_dataset(\n    data=initial_data,\n    path=delta_path,\n    mode=\"overwrite\"\n)\nprint(\"Initial Delta table created.\")\n\n# New data for an upsert: update Alice, add Charlie\nnew_data = pl.DataFrame({\n    \"id\": [1, 3],\n    \"name\": [\"Alicia\", \"Charlie\"],\n    \"version\": [2, 1]\n})\n\n# Perform a delta merge (upsert)\nfs.write_pydala_dataset(\n    data=new_data,\n    path=delta_path,\n    mode=\"delta\",\n    delta_subset=[\"id\"] # Column(s) to use for merging\n)\nprint(\"Delta merge completed.\")\n\n# Read the updated table\nupdated_df = fs.pydala_dataset(delta_path).to_polars()\nprint(\"Updated Delta table:\")\nprint(updated_df)\n# Expected: id=1 Alicia version=2, id=2 Bob version=1, id=3 Charlie version=1\n</code></pre> <p><code>fsspec-utils</code> facilitates integration with Delta Lake by providing <code>StorageOptions</code> that can be used to configure <code>deltalake</code>'s <code>DeltaTable</code> for various storage backends.</p> <p>This example demonstrates how to use <code>LocalStorageOptions</code> with <code>deltalake</code>'s <code>DeltaTable</code>. It shows how to initialize a <code>DeltaTable</code> instance by passing the <code>fsspec-utils</code> storage options, enabling seamless interaction with Delta Lake tables across different storage types.</p> <p>Step-by-step walkthrough:</p> <ol> <li>Create a temporary directory for our example</li> <li>Create a simple Polars DataFrame</li> <li>Write initial data to create the Delta table</li> <li>Create a LocalStorageOptions object for the temporary directory</li> <li>Create a DeltaTable instance, passing storage options</li> <li>Note: deltalake expects storage_options as a dict, which to_object_store_kwargs provides</li> <li>Read data from the DeltaTable</li> <li>Clean up the temporary directory</li> </ol> <p>Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It provides a reliable, scalable, and performant way to work with data lakes, combining the benefits of data lakes (low cost, flexibility) with data warehouses (reliability, performance).</p> <pre><code>from deltalake import DeltaTable\nfrom fsspec_utils.storage_options import LocalStorageOptions\nimport tempfile\nimport shutil\nimport os\nimport polars as pl\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ndelta_table_path = os.path.join(temp_dir, \"my_delta_table\")\nprint(f\"Creating a dummy Delta table at: {delta_table_path}\")\n\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"value\": [\"A\", \"B\", \"C\"]\n})\n\ndata.write_delta(delta_table_path, mode=\"overwrite\")\nprint(\"Initial data written to Delta table.\")\n\nlocal_options = LocalStorageOptions(path=temp_dir)\n\ndt = DeltaTable(delta_table_path, storage_options=local_options.to_object_store_kwargs())\nprint(f\"\\nSuccessfully created DeltaTable instance from: {delta_table_path}\")\nprint(f\"DeltaTable version: {dt.version()}\")\nprint(f\"DeltaTable files: {dt.files()}\")\n\ntable_data = dt.to_pyarrow_table()\nprint(\"\\nData read from DeltaTable:\")\nprint(table_data.to_pandas())\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\n</code></pre> <p>Key features of Delta Lake:</p> <ul> <li>ACID transactions: Ensures data integrity even with concurrent operations</li> <li>Time travel: Allows querying data as it existed at any point in time</li> <li>Schema enforcement: Maintains data consistency with schema validation</li> <li>Scalable metadata: Handles billions of files efficiently</li> <li>Unified analytics: Supports both batch and streaming workloads</li> </ul> <p>Integrating fsspec-utils with Delta Lake:</p> <p>The <code>fsspec-utils</code> <code>StorageOptions</code> classes can be used to configure <code>deltalake</code>'s <code>DeltaTable</code> for various storage backends. This integration allows you to:</p> <ol> <li>Use consistent configuration patterns across different storage systems</li> <li>Leverage the benefits of fsspec's unified filesystem interface</li> <li>Seamlessly switch between local and cloud storage without changing your Delta Lake code</li> </ol> <p>The <code>to_object_store_kwargs()</code> method converts <code>fsspec-utils</code> storage options into a dictionary format that <code>deltalake</code> expects for its <code>storage_options</code> parameter. This is necessary because <code>deltalake</code> requires storage options as a dictionary, while <code>fsspec-utils</code> provides them as structured objects.</p> <p>Step-by-step walkthrough:</p> <ol> <li> <p>Creating a temporary directory: We create a temporary directory to store our Delta table, ensuring the example is self-contained and doesn't leave artifacts on your system.</p> </li> <li> <p>Creating sample data: We create a simple Polars DataFrame with sample data that will be written to our Delta table.</p> </li> <li> <p>Writing to Delta table: Using the <code>write_delta</code> method, we convert our DataFrame into a Delta table. This creates the necessary Delta Lake metadata alongside the data files.</p> </li> <li> <p>Configuring storage options: We create a <code>LocalStorageOptions</code> object that points to our temporary directory. This object contains all the information needed to access the Delta table.</p> </li> <li> <p>Initializing DeltaTable: We create a <code>DeltaTable</code> instance by passing the table path and the storage options converted to a dictionary via <code>to_object_store_kwargs()</code>. This allows <code>deltalake</code> to locate and access the Delta table files.</p> </li> <li> <p>Verifying the DeltaTable: We check the version and files of our Delta table to confirm it was created correctly. Delta tables maintain version history, allowing you to track changes over time.</p> </li> <li> <p>Reading data: Finally, we read the data from our Delta table back into a PyArrow Table, demonstrating that we can successfully interact with the Delta Lake table using the fsspec-utils configuration.</p> </li> </ol> <p>This integration is particularly valuable when working with Delta Lake in cloud environments, as it allows you to use the same configuration approach for local development and production deployments across different cloud providers.</p>"},{"location":"advanced/#storage-options-management","title":"Storage Options Management","text":"<p><code>fsspec-utils</code> provides a robust system for managing storage configurations, simplifying credential handling and environment setup.</p>"},{"location":"advanced/#loading-from-environment-variables","title":"Loading from Environment Variables","text":"<p>Instead of hardcoding credentials, you can load storage options directly from environment variables.</p> <p>Example: Loading AWS S3 Configuration from Environment</p> <p>Set these environment variables before running your script: <pre><code>export AWS_ACCESS_KEY_ID=\"YOUR_ACCESS_KEY\"\nexport AWS_SECRET_ACCESS_KEY=\"YOUR_SECRET_KEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n</code></pre></p> <p>Then in Python: <pre><code>from fsspec_utils.storage_options import AwsStorageOptions\n\n# Load AWS options directly from environment variables\naws_opts = AwsStorageOptions.from_env()\nprint(f\"Loaded AWS region: {aws_opts.region}\")\n\n# Use it to create a filesystem\n# fs = aws_opts.to_filesystem()\n</code></pre></p>"},{"location":"advanced/#merging-storage-options","title":"Merging Storage Options","text":"<p>Combine multiple storage option configurations, useful for layering default settings with user-specific overrides.</p> <p>Example: Merging S3 Options</p> <pre><code>from fsspec_utils.storage_options import AwsStorageOptions, merge_storage_options\n\n# Base configuration\nbase_opts = AwsStorageOptions(\n    protocol=\"s3\",\n    region=\"us-east-1\",\n    access_key_id=\"DEFAULT_KEY\"\n)\n\n# User-provided overrides\nuser_overrides = {\n    \"access_key_id\": \"USER_KEY\",\n    \"allow_http\": True # New setting\n}\n\n# Merge, with user_overrides taking precedence\nmerged_opts = merge_storage_options(base_opts, user_overrides, overwrite=True)\n\nprint(f\"Merged Access Key ID: {merged_opts.access_key_id}\") # USER_KEY\nprint(f\"Merged Region: {merged_opts.region}\") # us-east-1\nprint(f\"Allow HTTP: {merged_opts.allow_http}\") # True\n</code></pre>"},{"location":"advanced/#note-on-github-examples","title":"Note on GitHub Examples","text":"<p>For a comprehensive collection of executable examples demonstrating various functionalities and advanced patterns of <code>fsspec-utils</code>, including those discussed in this document, please refer to the examples directory on GitHub. Each example is designed to be runnable and provides detailed insights into practical usage.</p>"},{"location":"advanced/#performance-tips","title":"Performance Tips","text":"<ul> <li>Caching: Always consider using <code>cached=True</code> with the <code>filesystem</code> function, especially for remote filesystems, to minimize repeated downloads.</li> <li>Parallel Reading: For multiple files, set <code>use_threads=True</code> in <code>read_json</code>, <code>read_csv</code>, and <code>read_parquet</code> to leverage concurrent I/O.</li> <li>Batch Processing: When dealing with a very large number of files or extremely large individual files, use the <code>batch_size</code> parameter in reading functions to process data in chunks, reducing memory footprint.</li> <li><code>opt_dtypes</code>: Utilize <code>opt_dtypes=True</code> in reading functions when converting to Polars or PyArrow to automatically optimize column data types, leading to more efficient memory usage and faster subsequent operations.</li> <li>Parquet Datasets: For large, partitioned Parquet datasets, use <code>pyarrow_dataset</code> or <code>pydala_dataset</code>. These leverage PyArrow's dataset API for efficient metadata handling, partition pruning, and predicate pushdown, reading only the necessary data.</li> <li>Compression: When writing Parquet files, choose an appropriate compression codec (e.g., <code>zstd</code>, <code>snappy</code>) to reduce file size and improve I/O performance. <code>zstd</code> often provides a good balance of compression ratio and speed.</li> </ul>"},{"location":"advanced/#flexible-storage-configuration","title":"Flexible Storage Configuration","text":"<p><code>fsspec-utils</code> simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using <code>StorageOptions</code> classes. These options can then be converted into <code>fsspec</code> filesystems.</p>"},{"location":"advanced/#local-storage-example","title":"Local Storage Example","text":"<p>This example demonstrates how to initialize <code>LocalStorageOptions</code> and use it to interact with the local filesystem.</p> <p>Step-by-step walkthrough:</p> <ol> <li>Create a temporary directory for our test</li> <li>Create a test file and write content to it</li> <li>List files in the directory to verify our file was created</li> <li>Read the content back to verify it was written correctly</li> <li>Clean up the temporary directory</li> </ol> <p>StorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.</p> <pre><code>import os\nimport tempfile\nimport shutil\nfrom fsspec_utils.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n</code></pre>"},{"location":"advanced/#conceptual-aws-s3-configuration","title":"Conceptual AWS S3 Configuration","text":"<p>This example demonstrates the configuration pattern for <code>AwsStorageOptions</code>. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <p>Note: The <code>to_filesystem()</code> method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library. <pre><code>from fsspec_utils.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n</code></pre></p>"},{"location":"advanced/#conceptual-azure-configuration","title":"Conceptual Azure Configuration","text":"<p>This example shows how to configure <code>AzureStorageOptions</code>.  It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <pre><code>from fsspec_utils.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n</code></pre>"},{"location":"advanced/#conceptual-gcs-configuration","title":"Conceptual GCS Configuration","text":"<p>This example shows how to configure <code>GcsStorageOptions</code>.  It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.</p> <p>StorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.</p> <p>The <code>to_filesystem()</code> method converts these options into <code>fsspec</code> compatible objects, enabling seamless integration with any fsspec-compatible library or tool.</p> <p>Important Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:</p> <ol> <li>The endpoint URLs are not real service endpoints</li> <li>The credentials are placeholder values that don't correspond to actual accounts</li> <li>The connection strings and tokens are examples, not valid credentials</li> </ol> <p>This approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.</p> <p>```python from fsspec_utils.storage_options import GcsStorageOptions</p> <p>print(\"=== Conceptual GcsStorageOptions Example (using a dummy token path) ===\\n\") gcs_options = GcsStorageOptions(     protocol=\"gs\",     project=\"demo-project\",     token=\"path/to/dummy-service-account.json\" )</p> <p>gcs_fs = gcs_options.to_filesystem() print(f\"Created fsspec filesystem for GCS: {type(gcs_fs).name}\") print(\"GCS storage example completed.\\n\")</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p><code>fsspec-utils</code> is designed to extend and enhance the capabilities of <code>fsspec</code>, providing a robust and flexible framework for interacting with various filesystems and data formats. Its architecture is modular, built around core components that abstract away complexities and offer specialized functionalities.</p>"},{"location":"architecture/#extending-fsspec","title":"Extending <code>fsspec</code>","text":"<p>At its core, <code>fsspec-utils</code> builds upon the <code>fsspec</code> (Filesystem Spec) library, which provides a unified Pythonic interface to various storage backends. <code>fsspec-utils</code> extends this functionality by:</p> <ul> <li>Simplifying Storage Configuration: It offers <code>StorageOptions</code> classes for various cloud providers (AWS S3, Google Cloud Storage, Azure Storage) and Git platforms (GitHub, GitLab), allowing for easier and more consistent configuration of filesystem access.</li> <li>Enhancing I/O Operations: It provides extended read/write capabilities for common data formats like JSON, CSV, and Parquet, with integrations for high-performance libraries like Polars and PyArrow.</li> <li>Improving Caching: The library includes an enhanced caching mechanism that preserves directory structures and offers better monitoring.</li> </ul>"},{"location":"architecture/#core-components","title":"Core Components","text":"<p>The <code>fsspec-utils</code> library is organized into several key modules:</p>"},{"location":"architecture/#core","title":"<code>core</code>","text":"<p>This module contains the fundamental extensions to <code>fsspec</code>. It includes the <code>filesystem</code> function, which acts as a central factory for creating <code>fsspec</code> compatible filesystem objects, potentially with enhanced features like caching and extended I/O. The <code>DirFileSystem</code> class is also part of this module, providing specialized handling for directory-based filesystems.</p>"},{"location":"architecture/#storage_options","title":"<code>storage_options</code>","text":"<p>This module is dedicated to managing storage configurations for different backends. It defines various <code>StorageOptions</code> classes (e.g., <code>AwsStorageOptions</code>, <code>GcsStorageOptions</code>, <code>AzureStorageOptions</code>, <code>GitHubStorageOptions</code>, <code>GitLabStorageOptions</code>) that encapsulate the necessary parameters for connecting to specific storage services. It also includes utility functions for inferring protocols from URIs and merging storage options.</p>"},{"location":"architecture/#utils","title":"<code>utils</code>","text":"<p>The <code>utils</code> module provides a collection of general-purpose utility functions that support various operations within <code>fsspec-utils</code>. These include:</p> <ul> <li>Parallel Processing: Functions like <code>run_parallel</code> for executing tasks concurrently.</li> <li>Type Conversion: Utilities such as <code>dict_to_dataframe</code> and <code>to_pyarrow_table</code> for data manipulation.</li> <li>Logging: A setup for consistent logging across the library.</li> <li>PyArrow and Polars Integration: A lot of utility functions, e.g. for optimizing data types and schemas when working with PyArrow tables and Polars DataFrames.</li> </ul>"},{"location":"architecture/#diagrams","title":"Diagrams","text":"<pre><code>graph TD\n    A[fsspec-utils] --&gt; B(Core Module)\n    A --&gt; C(Storage Options Module)\n    A --&gt; D(Utils Module)\n    B --&gt; E[Extends fsspec]\n    C --&gt; F{Cloud Providers}\n    C --&gt; G{Git Platforms}\n    D --&gt; H[Parallel Processing]\n    D --&gt; I[Type Conversion]\n    D --&gt; J[Logging]\n    D --&gt; K[PyArrow/Polars Integration]\n    F --&gt; L(AWS S3)\n    F --&gt; M(Google Cloud Storage)\n    F --&gt; N(Azure Storage)\n    G --&gt; O(GitHub)\n    G --&gt; P(GitLab)</code></pre>"},{"location":"contributing/","title":"Contributing to fsspec-utils","text":"<p>We welcome contributions to <code>fsspec-utils</code>! Your help makes this project better. This guide outlines how you can contribute, from reporting issues to submitting pull requests.</p>"},{"location":"contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.</p> <p>When reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your <code>fsspec-utils</code> version and Python environment details.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>We gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:</p> <ol> <li>Fork the Repository: Start by forking the <code>fsspec-utils</code> repository on GitHub.</li> <li>Clone Your Fork: Clone your forked repository to your local machine.     <pre><code>git clone https://github.com/your-username/fsspec-utils.git\ncd fsspec-utils\n</code></pre></li> <li>Create a New Branch: Create a new branch for your changes.     <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/issue-description\n</code></pre></li> <li>Make Your Changes: Implement your bug fix or feature.</li> <li>Write Tests: Ensure your changes are covered by appropriate unit tests.</li> <li>Run Tests: Verify all tests pass before submitting.     <pre><code>uv run pytest\n</code></pre></li> <li>Format Code: Ensure your code adheres to the project's style guidelines. The project uses <code>ruff</code> for linting and formatting.     <pre><code>uv run ruff check . --fix\nuv run ruff format .\n</code></pre></li> <li>Commit Your Changes: Write clear and concise commit messages.     <pre><code>git commit -m \"feat: Add new awesome feature\"\n</code></pre></li> <li>Push to Your Fork: Push your branch to your forked repository.     <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Open a Pull Request: Go to the original <code>fsspec-utils</code> repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>To set up your development environment, follow these steps:</p> <ol> <li>Clone the repository:     <pre><code>git clone https://github.com/fsspec/fsspec-utils.git\ncd fsspec-utils\n</code></pre></li> <li>Install <code>uv</code>:     <code>fsspec-utils</code> uses <code>uv</code> for dependency management and running commands. If you don't have <code>uv</code> installed, you can install it via <code>pip</code>:     <pre><code>pip install uv\n</code></pre></li> <li>Install Development Dependencies:     The project uses <code>uv</code> to manage dependencies. Install the <code>dev</code> dependency group which includes tools for testing, linting, and documentation generation.     <pre><code>uv pip install -e \".[dev]\"\n</code></pre>     This command installs the project in editable mode (<code>-e</code>) and includes all development-related dependencies specified in <code>pyproject.toml</code> under the <code>[project.optional-dependencies] dev</code> section.</li> </ol>"},{"location":"contributing/#best-practices-for-contributions","title":"Best Practices for Contributions","text":"<ul> <li>Code Style: Adhere to the existing code style. We use <code>ruff</code> for linting and formatting.</li> <li>Testing: All new features and bug fixes should be accompanied by relevant unit tests.</li> <li>Documentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.</li> <li>Commit Messages: Write descriptive commit messages that explain the purpose of your changes.</li> <li>Atomic Commits: Try to keep your commits focused on a single logical change.</li> <li>Branch Naming: Use clear and concise branch names (e.g., <code>feature/new-feature</code>, <code>bugfix/fix-issue-123</code>).</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p><code>fsspec-utils</code> can be installed using <code>pip</code>, the Python package installer.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher is required.</li> </ul>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"<p>To install <code>fsspec-utils</code> using <code>pip</code>, run the following command:</p> <pre><code>pip install fsspec-utils\n</code></pre>"},{"location":"installation/#upgrading","title":"Upgrading","text":"<p>To upgrade <code>fsspec-utils</code> to the latest version, use:</p> <pre><code>pip install --upgrade fsspec-utils\n</code></pre>"},{"location":"installation/#environment-management-with-uv-and-pixi","title":"Environment Management with <code>uv</code> and <code>pixi</code>","text":"<p>For robust dependency management and faster installations, we recommend using <code>uv</code> or <code>pixi</code>.</p>"},{"location":"installation/#using-uv","title":"Using <code>uv</code>","text":"<p><code>uv</code> is a fast Python package installer and resolver. To install <code>fsspec-utils</code> with <code>uv</code>:</p> <pre><code>uv pip install fsspec-utils\n</code></pre>"},{"location":"installation/#using-pixi","title":"Using <code>pixi</code>","text":"<p><code>pixi</code> is a modern package manager for Python and other languages. To add <code>fsspec-utils</code> to your <code>pixi</code> project:</p> <pre><code>pixi add fsspec-utils\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, consider the following:</p> <ul> <li>Python Version: Ensure you are using Python 3.8 or higher. You can check your Python version with <code>python --version</code>.</li> <li>Virtual Environments: It is highly recommended to use a virtual environment (e.g., <code>venv</code>, <code>conda</code>, <code>uv</code>, <code>pixi</code>) to avoid conflicts with system-wide packages.</li> <li>Permissions: If you encounter permission errors, you might need to run the installation command with <code>sudo</code> (e.g., <code>sudo pip install fsspec-utils</code>), but this is generally not recommended in a virtual environment.</li> <li>Network Issues: Check your internet connection if the installation fails to download packages.</li> </ul> <p>For further assistance, please refer to the official fsspec-utils GitHub repository or open an issue.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with <code>fsspec-utils</code> by demonstrating how to create and interact with a directory-based filesystem for local paths.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>First, ensure you have <code>fsspec-utils</code> installed.</p> <pre><code>pip install fsspec-utils\n</code></pre>"},{"location":"quickstart/#basic-usage-local-directory-filesystem","title":"Basic Usage: Local Directory FileSystem","text":"<p><code>fsspec-utils</code> simplifies working with various file systems by providing a unified interface. Here, we'll create a <code>DirFileSystem</code> for a local directory.</p> <p>The <code>filesystem</code> function from <code>fsspec_utils</code> allows you to instantiate a file system object. By setting <code>dirfs=True</code>, you indicate that you want a directory-based filesystem, which treats directories as files themselves.</p> <p>Let's create a local directory and then instantiate a <code>DirFileSystem</code> for it:</p> <pre><code>import os\nfrom fsspec_utils import filesystem\n\n# Define a local directory path\nlocal_dir_path = \"./my_local_data/\"\n\n# Ensure the directory exists\nos.makedirs(local_dir_path, exist_ok=True)\n\n# Create a DirFileSystem for the local path\nfs_dir_local = filesystem(local_dir_path, dirfs=True)\n\nprint(f\"Local DirFileSystem created: {fs_dir_local}\")\n\n# You can now use the fs_dir_local object to interact with the directory\n# For example, to list its contents (initially empty)\nprint(f\"Contents of {local_dir_path}: {fs_dir_local.ls('/')}\")\n\n# Let's create a dummy file inside the directory\nwith fs_dir_local.open(\"test_file.txt\", \"w\") as f:\n    f.write(\"Hello, fsspec-utils!\")\n\nprint(f\"Contents after creating test_file.txt: {fs_dir_local.ls('/')}\")\n\n# Read the content of the dummy file\nwith fs_dir_local.open(\"test_file.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content of test_file.txt: {content}\")\n\n# Clean up the created directory and file\nfs_dir_local.rm(\"test_file.txt\")\nos.rmdir(local_dir_path)\nprint(f\"Cleaned up {local_dir_path}\")\n</code></pre>"},{"location":"quickstart/#explanation","title":"Explanation","text":"<ol> <li><code>import os</code> and <code>from fsspec_utils import filesystem</code>: We import the necessary modules. <code>os</code> is used here to ensure the local directory exists, and <code>filesystem</code> is the core function from <code>fsspec-utils</code>.</li> <li><code>local_dir_path = \"./my_local_data/\"</code>: We define a relative path for our local directory.</li> <li><code>os.makedirs(local_dir_path, exist_ok=True)</code>: This line creates the <code>my_local_data</code> directory if it doesn't already exist.</li> <li><code>fs_dir_local = filesystem(local_dir_path, dirfs=True)</code>: This is where <code>fsspec-utils</code> comes into play. We create a <code>DirFileSystem</code> instance pointing to our local directory. The <code>dirfs=True</code> argument is crucial for enabling directory-level operations.</li> <li><code>fs_dir_local.ls('/')</code>: We use the <code>ls</code> method of our <code>fs_dir_local</code> object to list the contents of the root of our <code>my_local_data</code> directory. Initially, it will be empty.</li> <li><code>fs_dir_local.open(\"test_file.txt\", \"w\")</code>: We demonstrate writing a file within our <code>DirFileSystem</code> using the <code>open</code> method, similar to Python's built-in <code>open</code>.</li> <li><code>fs_dir_local.open(\"test_file.txt\", \"r\")</code>: We demonstrate reading the content of the file we just created.</li> <li><code>fs_dir_local.rm(\"test_file.txt\")</code> and <code>os.rmdir(local_dir_path)</code>: Finally, we clean up by removing the created file and the directory.</li> </ol> <p>This example provides a basic overview of how to use <code>fsspec-utils</code> to interact with a local directory as a filesystem. The same <code>filesystem</code> function can be used for various other storage backends like S3, GCS, HDFS, etc., by simply changing the path and providing appropriate <code>storage_options</code>.</p>"},{"location":"api/","title":"<code>fsspec-utils</code> API Reference","text":"<p>Welcome to the <code>fsspec-utils</code> API reference documentation. This section provides detailed information on the various modules, classes, and functions available in the library.</p>"},{"location":"api/#core-modules","title":"Core Modules","text":"<ul> <li> <p><code>fsspec_utils.core.base</code></p> </li> <li> <p><code>fsspec_utils.core.ext</code></p> </li> </ul>"},{"location":"api/#storage-options","title":"Storage Options","text":"<ul> <li> <p><code>fsspec_utils.storage_options.base</code></p> </li> <li> <p><code>fsspec_utils.storage_options.cloud</code></p> </li> <li> <p><code>fsspec_utils.storage_options.core</code></p> </li> <li> <p><code>fsspec_utils.storage_options.git</code></p> </li> </ul>"},{"location":"api/#utilities","title":"Utilities","text":"<ul> <li> <p><code>fsspec_utils.utils.datetime</code></p> </li> <li> <p><code>fsspec_utils.utils.logging</code></p> </li> <li> <p><code>fsspec_utils.utils.misc</code></p> </li> <li> <p><code>fsspec_utils.utils.polars</code></p> </li> <li> <p><code>fsspec_utils.utils.pyarrow</code></p> </li> <li> <p><code>fsspec_utils.utils.sql</code></p> </li> <li> <p><code>fsspec_utils.utils.types</code></p> </li> </ul>"},{"location":"api/fsspec_utils.core.base/","title":"<code>fsspec_utils.core.base</code> API Documentation","text":"<p>This module provides core filesystem functionalities and utilities, including custom cache mappers, enhanced cached filesystems, and a GitLab filesystem implementation.</p>"},{"location":"api/fsspec_utils.core.base/#filenamecachemapper","title":"<code>FileNameCacheMapper</code>","text":"<p>Maps remote file paths to local cache paths while preserving directory structure.</p> <p>This cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.</p> <p>Attributes:</p> <ul> <li><code>directory</code> (<code>str</code>): Base directory for cached files</li> </ul> <p>Example:</p> <pre><code>from fsspec_utils.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#__init__","title":"<code>__init__()</code>","text":"<p>Initialize cache mapper with base directory.</p> Parameter Type Description <code>directory</code> <code>str</code> Base directory where cached files will be stored"},{"location":"api/fsspec_utils.core.base/#__call__","title":"<code>__call__()</code>","text":"<p>Map remote file path to cache file path.</p> <p>Creates necessary subdirectories in the cache directory to maintain the original path structure.</p> Parameter Type Description <code>path</code> <code>str</code> Original file path from remote filesystem Returns Type Description <code>str</code> <code>str</code> Cache file path that preserves original structure <p>Example:</p> <pre><code>from fsspec_utils.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#monitoredsimplecachefilesystem","title":"<code>MonitoredSimpleCacheFileSystem</code>","text":"<p>Enhanced caching filesystem with monitoring and improved path handling.</p> <p>This filesystem extends <code>SimpleCacheFileSystem</code> to provide:</p> <ul> <li>Verbose logging of cache operations</li> <li>Improved path mapping for cache files</li> <li>Enhanced synchronization capabilities</li> <li>Better handling of parallel operations</li> </ul> <p>Attributes:</p> <ul> <li><code>_verbose</code> (<code>bool</code>): Whether to print verbose cache operations</li> <li><code>_mapper</code> (<code>FileNameCacheMapper</code>): Maps remote paths to cache paths</li> <li><code>storage</code> (<code>list[str]</code>): List of cache storage locations</li> <li><code>fs</code> (<code>AbstractFileSystem</code>): Underlying filesystem being cached</li> </ul> <p>Example:</p> <pre><code>from fsspec import filesystem\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#__init___1","title":"<code>__init__()</code>","text":"<p>Initialize monitored cache filesystem.</p> Parameter Type Description <code>fs</code> <code>Optional[fsspec.AbstractFileSystem]</code> Underlying filesystem to cache. If None, creates a local filesystem. <code>cache_storage</code> <code>Union[str, list[str]]</code> Cache storage location(s). Can be string path or list of paths. <code>verbose</code> <code>bool</code> Whether to enable verbose logging of cache operations. <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>SimpleCacheFileSystem</code>. <p>Example:</p> <pre><code># Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#_check_cache","title":"<code>_check_cache()</code>","text":"<p>Check if file exists in cache and return cache path if found.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path to check Returns Type Descript <code>Optional[str]</code> <code>str</code> or <code>None</code> Cache file path if found, None otherwise <p>Example:</p> <pre><code>from fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#_check_file","title":"<code>_check_file()</code>","text":"<p>Ensure file is in cache, downloading if necessary.</p> Parameter Type Description <code>path</code> <code>str</code> Remote file path Returns Type Description <code>str</code> <code>str</code> Local cache path for the file <p>Example:</p> <pre><code>from fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#gitlabfilesystem","title":"<code>GitLabFileSystem</code>","text":"<p>Filesystem interface for GitLab repositories.</p> <p>Provides read-only access to files in GitLab repositories, including:</p> <ul> <li>Public and private repositories</li> <li>Self-hosted GitLab instances</li> <li>Branch/tag/commit selection</li> <li>Token-based authentication</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\"</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL</li> <li><code>project_id</code> (<code>str</code>): Project ID</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, commit)</li> <li><code>token</code> (<code>str</code>): Access token</li> <li><code>api_version</code> (<code>str</code>): API version</li> </ul> <p>Example:</p> <pre><code># Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#__init___2","title":"<code>__init__()</code>","text":"<p>Initialize GitLab filesystem.</p> Parameter Type Description <code>base_url</code> <code>str</code> GitLab instance URL <code>project_id</code> <code>Optional[Union[str, int]]</code> Project ID number <code>project_name</code> <code>Optional[str]</code> Project name/path (alternative to project_id) <code>ref</code> <code>str</code> Git reference (branch, tag, or commit SHA) <code>token</code> <code>Optional[str]</code> GitLab personal access token <code>api_version</code> <code>str</code> API version to use <p>| <code>**kwargs</code> | <code>Any</code> | Additional filesystem arguments |</p> Raises Type Description <code>ValueError</code> <code>ValueError</code> If neither <code>project_id</code> nor <code>project_name</code> is provided <p>Example:</p> <pre><code>from fsspec_utils.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#_get_file_content","title":"<code>_get_file_content()</code>","text":"<p>Get file content from GitLab API.</p> Parameter Type Description <code>path</code> <code>str</code> File path in repository Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes <p>Example:</p> <pre><code>from fsspec_utils.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n</code></pre> Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file doesn't exist <code>requests.HTTPError</code> <code>requests.HTTPError</code> For other HTTP errors"},{"location":"api/fsspec_utils.core.base/#_open","title":"<code>_open()</code>","text":"<p>Open file for reading.</p> Parameter Type Description <code>path</code> <code>str</code> File path to open <code>mode</code> <code>str</code> File mode (only 'rb' and 'r' supported) <code>block_size</code> <code>Optional[int]</code> Block size for reading (unused) <code>cache_options</code> <code>Optional[dict]</code> Cache options (unused) <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description File-like object File-like object File-like object for reading Raises Type Description <code>ValueError</code> <code>ValueError</code> If mode is not supported"},{"location":"api/fsspec_utils.core.base/#cat","title":"<code>cat()</code>","text":"<p>Get file contents as bytes.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bytes</code> <code>bytes</code> File content as bytes"},{"location":"api/fsspec_utils.core.base/#ls","title":"<code>ls()</code>","text":"<p>List directory contents.</p> Parameter Type Description <code>path</code> <code>str</code> Directory path to list <code>detail</code> <code>bool</code> Whether to return detailed information <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>list</code> <code>list</code> List of files/directories or their details"},{"location":"api/fsspec_utils.core.base/#exists","title":"<code>exists()</code>","text":"<p>Check if file or directory exists.</p> Parameter Type Description <code>path</code> <code>str</code> Path to check <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>bool</code> <code>bool</code> True if path exists, False otherwise"},{"location":"api/fsspec_utils.core.base/#info","title":"<code>info()</code>","text":"<p>Get file information.</p> Parameter Type Description <code>path</code> <code>str</code> File path <code>**kwargs</code> <code>Any</code> Additional options Returns Type Description <code>dict</code> <code>dict</code> Dictionary with file information Raises Type Description <code>FileNotFoundError</code> <code>FileNotFoundError</code> If file not found"},{"location":"api/fsspec_utils.core.base/#filesystem","title":"<code>filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>dirfs</code> <code>bool</code> Whether to wrap the filesystem in a <code>DirFileSystem</code>. Defaults to <code>True</code>. <code>base_fs</code> <code>AbstractFileSystem</code> An existing filesystem to wrap. <code>**kwargs</code> <code>Any</code> Additional filesystem arguments <p>| Ret | :------ | :--- | :---------- | | <code>AbstractFileSystem</code> | <code>fsspec.AbstractFileSystem</code> | Configured filesystem instance |</p> <p>Example:</p> <pre><code># Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspec_utils.core.base/#get_filesystem","title":"<code>get_filesystem()</code>","text":"<p>Get filesystem instance with enhanced configuration options.</p> <p>Deprecated</p> <p>Use <code>filesystem</code> instead. This function will be removed in a future version.</p> <p>Creates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.</p> Parameter Type Description <code>protocol_or_path</code> <code>str</code> Filesystem protocol (e.g., \"s3\", \"file\") or path with protocol prefix <code>storage_options</code> <code>Optional[Union[BaseStorageOptions, dict]]</code> Storage configuration as <code>BaseStorageOptions</code> instance or dict <code>cached</code> <code>bool</code> Whether to wrap filesystem in caching layer <code>cache_storage</code> <code>Optional[str]</code> Cache directory path (if <code>cached=True</code>) <code>verbose</code> <code>bool</code> Enable verbose logging for cache operations <code>**kwargs</code> <code>Any</code> Additional filesystem arguments Returns Type Description <code>fsspec.AbstractFileSystem</code> <code>fsspec.AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code># Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/","title":"<code>fsspec_utils.core.ext</code> API Documentation","text":"<p>This module provides extended functionalities for <code>fsspec.AbstractFileSystem</code>, including methods for reading and writing various file formats (JSON, CSV, Parquet) with advanced options like batch processing, parallelization, and data type optimization. It also includes functions for creating PyArrow and Pydala datasets.</p>"},{"location":"api/fsspec_utils.core.ext/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> Parameter Type Description <code>path</code> <code>str</code> Base path to convert. Can include wildcards (<code>*</code> or <code>**</code>). Examples: \"data/\", \"data/.json\", \"data/*\" <code>format</code> <code>str | None</code> File format to match (without dot). If None, inferred from path. Examples: \"json\", \"csv\", \"parquet\" Returns Type Description <code>str</code> <code>str</code> Glob pattern that matches files of specified format. Examples: \"data/**/.json\", \"data/.csv\" <p>Example:</p> <pre><code># Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_json_file","title":"<code>read_json_file()</code>","text":"<p>Read a single JSON file from any filesystem.</p> <p>A public wrapper around <code>_read_json_file</code> providing a clean interface for reading individual JSON files.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to JSON file to read <code>include_file_path</code> <code>bool</code> Whether to return dict with filepath as key <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format Returns Type Description <code>dict</code> or <code>list[dict]</code> <code>dict</code> or <code>list[dict]</code> Parsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If <code>include_file_path=True</code>, returns <code>{filepath: data}</code>. <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_json","title":"<code>read_json()</code>","text":"<p>Read JSON data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading JSON data with support for:</p> <ul> <li>Single file or multiple files</li> <li>Regular JSON or JSON Lines format</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>DataFrame conversion</li> <li>File path tracking</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Include source filepath in output <code>jsonlines</code> <code>bool</code> Whether to read as JSON Lines format <code>as_dataframe</code> <code>bool</code> Convert output to Polars DataFrame(s) <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to DataFrame conversion Returns Type Description <code>dict</code> or <code>list[dict]</code> or <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>dict</code>: Single JSON file as dictionary - <code>list[dict]</code>: Multiple JSON files as list of dictionaries - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of Dataframes (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_csv_file","title":"<code>read_csv_file()</code>","text":"<p>Read a single CSV file from any filesystem.</p> <p>Internal function that handles reading individual CSV files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to CSV file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> <code>pl.DataFrame</code> DataFrame containing CSV data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_csv","title":"<code>read_csv()</code>","text":"<p>Read CSV data from one or more files with powerful options.</p> <p>Provides a flexible interface for reading CSV files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel</li> <li>File path tracking</li> <li>Polars DataFrame output</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single DataFrame <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pl.read_csv()</code> Returns Type Description <code>pl.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>Generator</code> Various types depending on arguments: - <code>pl.DataFrame</code>: Single or concatenated DataFrame - <code>list[pl.DataFrame]</code>: List of DataFrames (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_parquet_file","title":"<code>read_parquet_file()</code>","text":"<p>Read a single Parquet file from any filesystem.</p> <p>Internal function that handles reading individual Parquet files and optionally adds the source filepath as a column.</p> Parameter Type Description <code>self</code> <code>AbstractFileSystem</code> Filesystem instance to use for reading <code>path</code> <code>str</code> Path to Parquet file <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame dtypes <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> <code>pa.Table</code> PyArrow Table containing Parquet data <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_parquet","title":"<code>read_parquet()</code>","text":"<p>Read Parquet data with advanced features and optimizations.</p> <p>Provides a high-performance interface for reading Parquet files with support for:</p> <ul> <li>Single file or multiple files</li> <li>Batch processing for large datasets</li> <li>Parallel processing</li> <li>File path tracking</li> <li>Automatic concatenation</li> <li>PyArrow Table output</li> </ul> <p>The function automatically uses optimal reading strategies:</p> <ul> <li>Direct dataset reading for simple cases</li> <li>Parallel processing for multiple files</li> <li>Batched reading for memory efficiency</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as a column <code>concat</code> <code>bool</code> Combine multiple files/batches into single Table <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional arguments passed to <code>pq.read_table()</code> Returns Type Description <code>pa.Table</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on arguments: - <code>pa.Table</code>: Single or concatenated Table - <code>list[pa.Table]</code>: List of Tables (if <code>concat=False</code>) - <code>Generator</code>: If <code>batch_size</code> set, yields batches of above types <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#read_files","title":"<code>read_files()</code>","text":"<p>Universal interface for reading data files of any supported format.</p> <p>A unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:</p> <ul> <li>Batch processing</li> <li>Parallel reading</li> <li>File path tracking</li> <li>Format-specific optimizations</li> </ul> Parameter Type Description <code>path</code> <code>str</code> or <code>list[str]</code> Path(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings <code>format</code> <code>str</code> File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files <code>batch_size</code> <code>int | None</code> If set, enables batch reading with this many files per batch <code>include_file_path</code> <code>bool</code> Add source filepath as column/field <code>concat</code> <code>bool</code> Combine multiple files/batches into single result <code>jsonlines</code> <code>bool</code> For JSON format, whether to read as JSON Lines <code>use_threads</code> <code>bool</code> Enable parallel file reading <code>verbose</code> <code>bool</code> Print progress information <code>opt_dtypes</code> <code>bool</code> Optimize DataFrame/Arrow Table dtypes for performance <code>**kwargs</code> <code>Any</code> Additional format-specific arguments Returns Type Description <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>Generator</code> Various types depending on format and arguments: - <code>pl.DataFrame</code>: For CSV and optionally JSON - <code>pa.Table</code>: For Parquet - <code>list[pl.DataFrame</code> or <code>pa.Table]</code>: Without concatenation - <code>Generator</code>: If <code>batch_size</code> set, yields batches <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#pyarrow_dataset","title":"<code>pyarrow_dataset()</code>","text":"<p>Create a PyArrow dataset from files in any supported format.</p> <p>Creates a dataset that provides optimized reading and querying capabilities including:</p> <ul> <li>Schema inference and enforcement</li> <li>Partition discovery and pruning</li> <li>Predicate pushdown</li> <li>Column projection</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Base path to dataset files <code>format</code> <code>str</code> File format. Currently supports: - \"parquet\" (default) - \"csv\" - \"json\" (experimental) <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional arguments for dataset creation Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#pyarrow_parquet_dataset","title":"<code>pyarrow_parquet_dataset()</code>","text":"<p>Create a PyArrow dataset optimized for Parquet files.</p> <p>Creates a dataset specifically for Parquet data, automatically handling <code>_metadata</code> files for optimized reading.</p> <p>This function is particularly useful for:</p> <ul> <li>Datasets with existing <code>_metadata</code> files</li> <li>Multi-file datasets that should be treated as one</li> <li>Partitioned Parquet datasets</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Path to dataset directory or <code>_metadata</code> file <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce. If None, inferred from data. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional dataset arguments Returns Type Description <code>pds.Dataset</code> <code>pds.Dataset</code> PyArrow dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &amp;\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#pydala_dataset","title":"<code>pydala_dataset()</code>","text":"<p>Create a Pydala dataset for advanced Parquet operations.</p> <p>Creates a dataset with additional features beyond PyArrow including:</p> <ul> <li>Delta table support</li> <li>Schema evolution</li> <li>Advanced partitioning</li> <li>Metadata management</li> <li>Sort key optimization</li> </ul> Parameter Type Description <code>path</code> <code>str</code> Path to dataset directory <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> How the dataset is partitioned. Can be: - <code>str</code>: Single partition field - <code>list[str]</code>: Multiple partition fields - <code>pds.Partitioning</code>: Custom partitioning scheme <code>**kwargs</code> <code>Any</code> Additional dataset configuration Returns Type Description <code>ParquetDataset</code> <code>ParquetDataset</code> Pydala dataset instance <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Create dataset\nds = fs.pydala_dataset(\n    \"data/\",\n    partitioning=[\"date\"]\n)\n\n# Write with delta support\nds.write_to_dataset(\n    new_data,\n    mode=\"delta\",\n    delta_subset=[\"id\"]\n)\n\n# Read with metadata\ndf = ds.to_polars()\nprint(df.columns)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#write_parquet","title":"<code>write_parquet()</code>","text":"<p>Write data to a Parquet file with automatic format conversion.</p> <p>Handles writing data from multiple input formats to Parquet with:</p> <ul> <li>Automatic conversion to PyArrow</li> <li>Schema validation/coercion</li> <li>Metadata collection</li> <li>Compression and encoding options</li> </ul> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pd.DataFrame</code> or <code>dict</code> or <code>list[dict]</code> Input data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts <code>path</code> <code>str</code> Output Parquet file path <code>schema</code> <code>pa.Schema | None</code> Optional schema to enforce on write <code>**kwargs</code> <code>Any</code> Additional arguments for <code>pq.write_table()</code> Returns Type Description <code>pq.FileMetaData</code> <code>pq.FileMetaData</code> Metadata of written Parquet file Raises Type Description <code>SchemaError</code> <code>SchemaError</code> If data doesn't match schema <code>ValueError</code> <code>ValueError</code> If data cannot be converted <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport numpy as np\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(1000),\n    \"value\": pl.Series(np.random.randn(1000))\n})\nmetadata = fs.write_parquet(\n    df,\n    \"data.parquet\",\n    compression=\"zstd\",\n    compression_level=3\n)\nprint(f\"Rows: {metadata.num_rows}\")\n\n# Write with schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"value\", pa.float64())\n])\nmetadata = fs.write_parquet(\n    {\"id\": [1, 2], \"value\": [0.1, 0.2]},\n    \"data.parquet\",\n    schema=schema\n)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#write_json","title":"<code>write_json()</code>","text":"<p>Write data to a JSON file with flexible input support.</p> <p>Handles writing data in various formats to JSON or JSON Lines, with optional appending for streaming writes.</p> Parameter Type Description <code>data</code> <code>dict</code> or <code>pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pd.DataFrame</code> or <code>dict</code> or <code>list[dict]</code> Input data in various formats: - Dict or list of dicts - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame <code>path</code> <code>str</code> Output JSON file path <code>append</code> <code>bool</code> Whether to append to existing file (JSON Lines mode) <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write dictionary\ndata = {\"name\": \"test\", \"values\": [1, 2, 3]}\nfs.write_json(data, \"config.json\")\n\n# Stream records\ndf1 = pl.DataFrame({\"id\": [1], \"value\": [\"first\"]})\ndf2 = pl.DataFrame({\"id\": [2], \"value\": [\"second\"]})\nfs.write_json(df1, \"stream.jsonl\", append=False)\nfs.write_json(df2, \"stream.jsonl\", append=True)\n\n# Convert PyArrow\ntable = pa.table({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\nfs.write_json(table, \"data.json\")\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#write_csv","title":"<code>write_csv()</code>","text":"<p>Write data to a CSV file with flexible input support.</p> <p>Handles writing data from multiple formats to CSV with options for:</p> <ul> <li>Appending to existing files</li> <li>Custom delimiters and formatting</li> <li>Automatic type conversion</li> <li>Header handling</li> </ul> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pd.DataFrame</code> or <code>dict</code> or <code>list[dict]</code> Input data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts <code>path</code> <code>str</code> Output CSV file path <code>append</code> <code>bool</code> Whether to append to existing file <code>**kwargs</code> <code>Any</code> Additional arguments for CSV writing: - <code>delimiter</code>: Field separator (default \",\") - <code>header</code>: Whether to write header row - <code>quote_char</code>: Character for quoting fields - <code>date_format</code>: Format for date/time fields - <code>float_precision</code>: Decimal places for floats <p>Example:</p> <pre><code>from fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nfrom datetime import datetime\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(100),\n    \"name\": [\"item_\" + str(i) for i in range(100)]\n})\nfs.write_csv(df, \"items.csv\")\n\n# Append records\nnew_items = pl.DataFrame({\n    \"id\": range(100, 200),\n    \"name\": [\"item_\" + str(i) for i in range(100, 200)]\n})\nfs.write_csv(\n    new_items,\n    \"items.csv\",\n    append=True,\n    header=False\n)\n\n# Custom formatting\ndata = pa.table({\n    \"date\": [datetime.now()],\n    \"value\": [123.456]\n})\nfs.write_csv(\n    data,\n    \"formatted.csv\",\n    date_format=\"%Y-%m-%d\",\n    float_precision=2\n)\n</code></pre>"},{"location":"api/fsspec_utils.core.ext/#write_file","title":"<code>write_file()</code>","text":"<p>Write a DataFrame to a file in the given format.</p> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pd.DataFrame</code> or <code>dict</code> Data to write. <code>path</code> <code>str</code> Path to write the data. <code>format</code> <code>str</code> Format of the file. <code>**kwargs</code> <code>Any</code> Additional keyword arguments. Returns Type Description <code>None</code> <code>None</code>"},{"location":"api/fsspec_utils.core.ext/#write_files","title":"<code>write_files()</code>","text":"<p>Write a DataFrame or a list of DataFrames to a file or a list of files.</p> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pa.RecordBatch</code> or <code>pa.RecordBatchReader</code> or <code>pd.DataFrame</code> or <code>dict</code> or <code>list[pl.DataFrame</code> or <code>pl.LazyFrame</code> or <code>pa.Table</code> or <code>pa.RecordBatch</code> or <code>pa.RecordBatchReader</code> or <code>pd.DataFrame</code> or <code>dict]</code> Data to write. <code>path</code> <code>str</code> or <code>list[str]</code> Path to write the data. <code>basename</code> <code>str</code> Basename of the files. Defaults to None. <code>format</code> <code>str</code> Format of the data. Defaults to None. <code>concat</code> <code>bool</code> If True, concatenate the DataFrames. Defaults to True. <code>unique</code> <code>bool</code> or <code>list[str]</code> or <code>str</code> If True, remove duplicates. Defaults to False. <code>mode</code> <code>str</code> Write mode. Defaults to 'append'. Options: 'append', 'overwrite', 'delete_matching', 'error_if_exists'. <code>use_threads</code> <code>bool</code> If True, use parallel processing. Defaults to True. <code>verbose</code> <code>bool</code> If True, print verbose output. Defaults to True. <code>**kwargs</code> <code>Any</code> Additional keyword arguments. Returns Type Description <code>None</code> <code>None</code> Raises Type Description <code>FileExistsError</code> <code>FileExistsError</code> If file already exists and mode is 'error_if_exists'."},{"location":"api/fsspec_utils.core.ext/#write_pyarrow_dataset","title":"<code>write_pyarrow_dataset()</code>","text":"<p>Write a tabular data to a PyArrow dataset.</p> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>pa.RecordBatch</code> or <code>pa.RecordBatchReader</code> or <code>pd.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>list[pa.RecordBatch]</code> or <code>list[pa.RecordBatchReader]</code> or <code>list[pd.DataFrame]</code> Data to write. <code>path</code> <code>str</code> Path to write the data. <code>basename</code> <code>str | None</code> Basename of the files. Defaults to None. <code>schema</code> <code>pa.Schema | None</code> Schema of the data. Defaults to None. <code>partition_by</code> <code>str</code> or <code>list[str]</code> or <code>pds.Partitioning</code> or <code>None</code> Partitioning of the data. Defaults to None. <code>partitioning_flavor</code> <code>str</code> Partitioning flavor. Defaults to 'hive'. <code>mode</code> <code>str</code> Write mode. Defaults to 'append'. <code>format</code> <code>str | None</code> Format of the data. Defaults to 'parquet'. <code>compression</code> <code>str</code> Compression algorithm. Defaults to 'zstd'. <code>max_rows_per_file</code> <code>int | None</code> Maximum number of rows per file. Defaults to 2,500,000. <code>row_group_size</code> <code>int | None</code> Row group size. Defaults to 250,000. <code>concat</code> <code>bool</code> If True, concatenate the DataFrames. Defaults to True. <code>unique</code> <code>bool</code> or <code>str</code> or <code>list[str]</code> If True, remove duplicates. Defaults to False. <code>**kwargs</code> <code>Any</code> Additional keyword arguments for <code>pds.write_dataset</code>. Returns Type Description <code>list[pq.FileMetaData]</code> or <code>None</code> List of Parquet file metadata or None."},{"location":"api/fsspec_utils.core.ext/#write_pydala_dataset","title":"<code>write_pydala_dataset()</code>","text":"<p>Write a tabular data to a Pydala dataset.</p> Parameter Type Description <code>data</code> <code>pl.DataFrame</code> or <code>pa.Table</code> or <code>pa.RecordBatch</code> or <code>pa.RecordBatchReader</code> or <code>pd.DataFrame</code> or <code>list[pl.DataFrame]</code> or <code>list[pa.Table]</code> or <code>list[pa.RecordBatch]</code> or <code>list[pa.RecordBatchReader]</code> or <code>list[pd.DataFrame]</code> Data to write. <code>path</code> <code>str</code> Path to write the data. <code>mode</code> <code>str</code> Write mode. Defaults to 'append'. Options: 'delta', 'overwrite'. <code>basename</code> <code>str | None</code> Basename of the files. Defaults to None. <code>partition_by</code> <code>str</code> or <code>list[str]</code> or <code>None</code> Partitioning of the data. Defaults to None. <code>partitioning_flavor</code> <code>str</code> Partitioning flavor. Defaults to 'hive'. <code>max_rows_per_file</code> <code>int | None</code> Maximum number of rows per file. Defaults to 2,500,000. <code>row_group_size</code> <code>int | None</code> Row group size. Defaults to 250,000. <code>compression</code> <code>str</code> Compression algorithm. Defaults to 'zstd'. <code>sort_by</code> <code>str</code> or <code>list[str]</code> or <code>list[tuple[str, str]]</code> or <code>None</code> Columns to sort by. Defaults to None. <code>unique</code> <code>bool</code> or <code>str</code> or <code>list[str]</code> If True, ensure unique values. Defaults to False. <code>delta_subset</code> <code>str</code> or <code>list[str]</code> or <code>None</code> Subset of columns to include in delta table. Defaults to None. <code>update_metadata</code> <code>bool</code> If True, update metadata. Defaults to True. <code>alter_schema</code> <code>bool</code> If True, alter schema. Defaults to False. <code>timestamp_column</code> <code>str</code> or <code>None</code> Timestamp column. Defaults to None. <code>verbose</code> <code>bool</code> If True, print verbose output. Defaults to True. <code>**kwargs</code> <code>Any</code> Additional keyword arguments for <code>ParquetDataset.write_to_dataset</code>. Returns Type Description <code>None</code> <code>None</code>"},{"location":"api/fsspec_utils.storage_options.base/","title":"<code>fsspec_utils.storage_options.base</code> API Documentation","text":"<p>This module defines the base class for filesystem storage configuration options.</p>"},{"location":"api/fsspec_utils.storage_options.base/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including:</p> <ul> <li>YAML serialization/deserialization</li> <li>Dictionary conversion</li> <li>Filesystem instance creation</li> <li>Configuration updates</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.base/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> Parameter Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary Returns Type Description <code>dict</code> <code>dict</code> Dictionary of storage options with non-None values <p>Example:</p> <pre><code>from fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.base/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for reading file Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Loaded storage options instance <p>Example:</p> <pre><code># Load from local file\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.base/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> Parameter Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>AbstractFileSystem</code> Filesystem to use for writing <p>Example:</p> <pre><code>from fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.base/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance <p>Example:</p> <pre><code>from fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.base/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> Parameter Type Description <code>**kwargs</code> <code>Any</code> New option values to set Returns Type Description <code>BaseStorageOptions</code> <code>BaseStorageOptions</code> Updated instance <p>Example:</p> <pre><code>from fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/","title":"<code>fsspec_utils.storage_options.cloud</code> API Documentation","text":"<p>This module defines storage option classes for various cloud providers, including Azure, Google Cloud Storage (GCS), and Amazon Web Services (AWS) S3. These classes provide structured ways to configure access to cloud storage, supporting different authentication methods and specific cloud service parameters.</p>"},{"location":"api/fsspec_utils.storage_options.cloud/#azurestorageoptions","title":"<code>AzureStorageOptions</code>","text":"<p>Azure Storage configuration options.</p> <p>Provides configuration for Azure storage services:</p> <ul> <li>Azure Blob Storage (<code>az://</code>)</li> <li>Azure Data Lake Storage Gen2 (<code>abfs://</code>)</li> <li>Azure Data Lake Storage Gen1 (<code>adl://</code>)</li> </ul> <p>Supports multiple authentication methods:</p> <ul> <li>Connection string</li> <li>Account key</li> <li>Service principal</li> <li>Managed identity</li> <li>SAS token</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"az\", \"abfs\", or \"adl\")</li> <li><code>account_name</code> (<code>str</code>): Storage account name</li> <li><code>account_key</code> (<code>str</code>): Storage account access key</li> <li><code>connection_string</code> (<code>str</code>): Full connection string</li> <li><code>tenant_id</code> (<code>str</code>): Azure AD tenant ID</li> <li><code>client_id</code> (<code>str</code>): Service principal client ID</li> <li><code>client_secret</code> (<code>str</code>): Service principal client secret</li> <li><code>sas_token</code> (<code>str</code>): SAS token for limited access</li> </ul> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard Azure environment variables:</p> <ul> <li><code>AZURE_STORAGE_PROTOCOL</code></li> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code></li> <li><code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li><code>AZURE_STORAGE_CONNECTION_STRING</code></li> <li><code>AZURE_TENANT_ID</code></li> <li><code>AZURE_CLIENT_ID</code></li> <li><code>AZURE_CLIENT_SECRET</code></li> <li><code>AZURE_STORAGE_SAS_TOKEN</code></li> </ul> Returns Type Description <code>AzureStorageOptions</code> <code>AzureStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard Azure environment variables.</p> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#gcsstorageoptions","title":"<code>GcsStorageOptions</code>","text":"<p>Google Cloud Storage configuration options.</p> <p>Provides configuration for GCS access with support for:</p> <ul> <li>Service account authentication</li> <li>Default application credentials</li> <li>Token-based authentication</li> <li>Project configuration</li> <li>Custom endpoints</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol (\"gs\" or \"gcs\")</li> <li><code>token</code> (<code>str</code>): Path to service account JSON file</li> <li><code>project</code> (<code>str</code>): Google Cloud project ID</li> <li><code>access_token</code> (<code>str</code>): OAuth2 access token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom storage endpoint</li> <li><code>timeout</code> (<code>int</code>): Request timeout in seconds</li> </ul> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GCP environment variables:</p> <ul> <li><code>GOOGLE_CLOUD_PROJECT</code>: Project</li> <li><code>GOOGLE_APPLICATION_CREDENTIALS</code>: Service account file path</li> <li><code>STORAGE_EMULATOR_HOST</code>: Custom endpoint (for testing)</li> <li><code>GCS_OAUTH_TOKEN</code>: OAuth2 access token</li> </ul> Returns Type Description <code>GcsStorageOptions</code> <code>GcsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># With environment variables set:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GCP environment variables.</p> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for GCSFileSystem <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import GcsStorageOptions\nfrom fsspec_utils.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#awsstorageoptions","title":"<code>AwsStorageOptions</code>","text":"<p>AWS S3 storage configuration options.</p> <p>Provides comprehensive configuration for S3 access with support for:</p> <ul> <li>Multiple authentication methods (keys, profiles, environment)</li> <li>Custom endpoints for S3-compatible services</li> <li>Region configuration</li> <li>SSL/TLS settings</li> </ul> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"s3\" for S3 storage</li> <li><code>access_key_id</code> (<code>str</code>): AWS access key ID</li> <li><code>secret_access_key</code> (<code>str</code>): AWS secret access key</li> <li><code>session_token</code> (<code>str</code>): AWS session token</li> <li><code>endpoint_url</code> (<code>str</code>): Custom S3 endpoint URL</li> <li><code>region</code> (<code>str</code>): AWS region name</li> <li><code>allow_invalid_certificates</code> (<code>bool</code>): Skip SSL certificate validation</li> <li><code>allow_http</code> (<code>bool</code>): Allow unencrypted HTTP connections</li> </ul> <p>Example:</p> <pre><code># Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#create","title":"<code>create()</code>","text":"<p>Creates an <code>AwsStorageOptions</code> instance, handling aliases and profile loading.</p> Parameter Type Description <code>protocol</code> <code>str</code> Storage protocol, defaults to \"s3\". <code>access_key_id</code> <code>str | None</code> AWS access key ID. <code>secret_access_key</code> <code>str | None</code> AWS secret access key. <code>session_token</code> <code>str | None</code> AWS session token. <code>endpoint_url</code> <code>str | None</code> Custom S3 endpoint URL. <code>region</code> <code>str | None</code> AWS region name. <code>allow_invalid_certificates</code> <code>bool | None</code> Skip SSL certificate validation. <code>allow_http</code> <code>bool | None</code> Allow unencrypted HTTP connections. <code>key</code> <code>str | None</code> Alias for <code>access_key_id</code>. <code>secret</code> <code>str | None</code> Alias for <code>secret_access_key</code>. <code>token</code> <code>str | None</code> Alias for <code>session_token</code>. <code>profile</code> <code>str | None</code> AWS credentials profile name to load credentials from. Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> An initialized <code>AwsStorageOptions</code> instance."},{"location":"api/fsspec_utils.storage_options.cloud/#from_aws_credentials","title":"<code>from_aws_credentials()</code>","text":"<p>Create storage options from AWS credentials file.</p> <p>Loads credentials from <code>~/.aws/credentials</code> and <code>~/.aws/config</code> files.</p> Parameter Type Description <code>profile</code> <code>str</code> AWS credentials profile name <code>allow_invalid_certificates</code> <code>bool</code> Skip SSL certificate validation <code>allow_http</code> <code>bool</code> Allow unencrypted HTTP connections Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options Raises Type Description <code>ValueError</code> <code>ValueError</code> If profile not found <code>FileNotFoundError</code> <code>FileNotFoundError</code> If credentials files missing <p>Example:</p> <pre><code># Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#from_env_2","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard AWS environment variables:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_SESSION_TOKEN</code></li> <li><code>AWS_ENDPOINT_URL</code></li> <li><code>AWS_DEFAULT_REGION</code></li> <li><code>ALLOW_INVALID_CERTIFICATE</code></li> <li><code>AWS_ALLOW_HTTP</code></li> </ul> Returns Type Description <code>AwsStorageOptions</code> <code>AwsStorageOptions</code> Configured storage options <p>Example:</p> <pre><code># Load from environment\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for fsspec S3FileSystem <p>Example:</p> <pre><code>options = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Convert options to object store arguments.</p> Parameter Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support Returns Type Description <code>dict</code> <code>dict</code> Arguments suitable for object store clients <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_env_2","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard AWS environment variables.</p> <p>Example:</p> <pre><code>from fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n</code></pre>"},{"location":"api/fsspec_utils.storage_options.cloud/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> Returns Type Description <code>AbstractFileSystem</code> <code>AbstractFileSystem</code> Configured filesystem instance"},{"location":"api/fsspec_utils.storage_options.core/","title":"<code>fsspec_utils.storage_options.core</code> API Reference","text":""},{"location":"api/fsspec_utils.storage_options.core/#localstorageoptions","title":"<code>LocalStorageOptions</code>","text":"<p>Local filesystem configuration options.</p> <p>Provides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"file\" for local filesystem</li> <li><code>auto_mkdir</code> (<code>bool</code>): Create directories automatically</li> <li><code>mode</code> (<code>int</code>): Default file creation mode (unix-style)</li> </ul> <p>Example: <pre><code># Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for LocalFileSystem</li> </ul> <p>Example: <pre><code>options = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#from_dict","title":"<code>from_dict()</code>","text":"<p>Create appropriate storage options instance from dictionary.</p> <p>Factory function that creates the correct storage options class based on protocol.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\") <code>storage_options</code> <code>dict</code> Dictionary of configuration options <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Appropriate storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Factory function that creates and configures storage options from protocol-specific environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol identifier (e.g., \"s3\", \"github\") <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol is not supported</li> </ul> <p>Example: <pre><code># With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#infer_protocol_from_uri","title":"<code>infer_protocol_from_uri()</code>","text":"<p>Infer the storage protocol from a URI string.</p> <p>Analyzes the URI to determine the appropriate storage protocol based on the scheme or path format.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI or path string to analyze. Examples: - \"s3://bucket/path\" - \"gs://bucket/path\" - \"github://org/repo\" - \"/local/path\" <p>Returns:</p> <ul> <li><code>str</code>: Inferred protocol identifier</li> </ul> <p>Example: <pre><code># S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#storage_options_from_uri","title":"<code>storage_options_from_uri()</code>","text":"<p>Create storage options instance from a URI string.</p> <p>Infers the protocol and extracts relevant configuration from the URI to create appropriate storage options.</p> <p>Parameters:</p> Name Type Description <code>uri</code> <code>str</code> URI string containing protocol and optional configuration. Examples: - \"s3://bucket/path\" - \"gs://project/bucket/path\" - \"github://org/repo\" <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Configured storage options instance</li> </ul> <p>Example: <pre><code># S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#merge_storage_options","title":"<code>merge_storage_options()</code>","text":"<p>Merge multiple storage options into a single configuration.</p> <p>Combines options from multiple sources with control over precedence.</p> <p>Parameters:</p> Name Type Description <code>*options</code> <code>BaseStorageOptions</code> or <code>dict</code> Storage options to merge. Can be: - BaseStorageOptions instances - Dictionaries of options - None values (ignored) <code>overwrite</code> <code>bool</code> Whether later options override earlier ones <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Combined storage options</li> </ul> <p>Example: <pre><code># Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#storageoptions","title":"<code>StorageOptions</code>","text":"<p>High-level storage options container and factory.</p> <p>Provides a unified interface for creating and managing storage options for different protocols.</p> <p>Attributes:</p> <ul> <li><code>storage_options</code> (<code>BaseStorageOptions</code>): Underlying storage options instance</li> </ul> <p>Example: <pre><code># Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#create","title":"<code>create()</code>","text":"<p>Create storage options from arguments.</p> <p>Parameters:</p> Name Type Description <code>**data</code> <code>dict</code> Either: - protocol and configuration options - storage_options=pre-configured instance <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options instance</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If protocol missing or invalid</li> </ul> <p>Example: <pre><code># Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#from_yaml","title":"<code>from_yaml()</code>","text":"<p>Create storage options from YAML configuration.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem for reading configuration <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Parameters:</p> Name Type Description <code>protocol</code> <code>str</code> Storage protocol to configure <p>Returns:</p> <ul> <li><code>StorageOptions</code>: Environment-configured options</li> </ul> <p>Example: <pre><code># Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_filesystem","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_dict","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output <p>Returns:</p> <ul> <li><code>dict</code>: Storage options as dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_object_store_kwargs","title":"<code>to_object_store_kwargs()</code>","text":"<p>Get options formatted for object store clients.</p> <p>Parameters:</p> Name Type Description <code>with_conditional_put</code> <code>bool</code> Add etag-based conditional put support <p>Returns:</p> <ul> <li><code>dict</code>: Object store configuration dictionary</li> </ul> <p>Example: <pre><code>options = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#basestorageoptions","title":"<code>BaseStorageOptions</code>","text":"<p>Base class for filesystem storage configuration options.</p> <p>Provides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Storage protocol identifier (e.g., \"s3\", \"gs\", \"file\")</li> </ul> <p>Example: <pre><code># Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_dict_1","title":"<code>to_dict()</code>","text":"<p>Convert storage options to dictionary.</p> <p>Parameters:</p> Name Type Description <code>with_protocol</code> <code>bool</code> Whether to include protocol in output dictionary <p>Returns:</p> <ul> <li><code>dict</code>: Dictionary of storage options with non-None values</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#from_yaml_1","title":"<code>from_yaml()</code>","text":"<p>Load storage options from YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path to YAML configuration file <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for reading file <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Loaded storage options instance</li> </ul> <p>Example: <pre><code># Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_yaml","title":"<code>to_yaml()</code>","text":"<p>Save storage options to YAML file.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> Path where to save configuration <code>fs</code> <code>fsspec.AbstractFileSystem</code>, optional Filesystem to use for writing <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#to_filesystem_1","title":"<code>to_filesystem()</code>","text":"<p>Create fsspec filesystem instance from options.</p> <p>Returns:</p> <ul> <li><code>AbstractFileSystem</code>: Configured filesystem instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.core/#update","title":"<code>update()</code>","text":"<p>Update storage options with new values.</p> <p>Parameters:</p> Name Type Description <code>**kwargs</code> <code>dict</code> New option values to set <p>Returns:</p> <ul> <li><code>BaseStorageOptions</code>: Updated instance</li> </ul> <p>Example: <pre><code>options = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/","title":"<code>fsspec_utils.storage_options.git</code> API Reference","text":""},{"location":"api/fsspec_utils.storage_options.git/#githubstorageoptions","title":"<code>GitHubStorageOptions</code>","text":"<p>GitHub repository storage configuration options.</p> <p>Provides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"github\" for GitHub storage</li> <li><code>org</code> (<code>str</code>): Organization or user name</li> <li><code>repo</code> (<code>str</code>): Repository name</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA</li> <li><code>token</code> (<code>str</code>): GitHub personal access token</li> <li><code>api_url</code> (<code>str</code>): Custom GitHub API URL for enterprise instances</li> </ul> <p>Example: <pre><code># Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#from_env","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL</p> <p>Returns:</p> <ul> <li><code>GitHubStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#to_env","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitHub environment variables.</p> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#to_fsspec_kwargs","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitHubFileSystem</li> </ul> <p>Example: <pre><code>options = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#gitlabstorageoptions","title":"<code>GitLabStorageOptions</code>","text":"<p>GitLab repository storage configuration options.</p> <p>Provides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication</p> <p>Attributes:</p> <ul> <li><code>protocol</code> (<code>str</code>): Always \"gitlab\" for GitLab storage</li> <li><code>base_url</code> (<code>str</code>): GitLab instance URL, defaults to gitlab.com</li> <li><code>project_id</code> (<code>str</code> | <code>int</code>): Project ID number</li> <li><code>project_name</code> (<code>str</code>): Project name/path</li> <li><code>ref</code> (<code>str</code>): Git reference (branch, tag, or commit SHA)</li> <li><code>token</code> (<code>str</code>): GitLab personal access token</li> <li><code>api_version</code> (<code>str</code>): API version to use</li> </ul> <p>Example: <pre><code># Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#from_env_1","title":"<code>from_env()</code>","text":"<p>Create storage options from environment variables.</p> <p>Reads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version</p> <p>Returns:</p> <ul> <li><code>GitLabStorageOptions</code>: Configured storage options</li> </ul> <p>Example: <pre><code># With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#to_env_1","title":"<code>to_env()</code>","text":"<p>Export options to environment variables.</p> <p>Sets standard GitLab environment variables.</p> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n</code></pre></p>"},{"location":"api/fsspec_utils.storage_options.git/#to_fsspec_kwargs_1","title":"<code>to_fsspec_kwargs()</code>","text":"<p>Convert options to fsspec filesystem arguments.</p> <p>Returns:</p> <ul> <li><code>dict</code>: Arguments suitable for GitLabFileSystem</li> </ul> <p>Example: <pre><code>options = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.datetime/","title":"<code>fsspec_utils.utils.datetime</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.datetime/#get_timestamp_column","title":"<code>get_timestamp_column()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> Input DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspec_utils.utils.datetime import get_timestamp_column\n\ndf = pl.DataFrame({\n    \"timestamp_col\": [1678886400, 1678972800],\n    \"value\": [10, 20]\n})\ncol_name = get_timestamp_column(df)\nprint(col_name)\n# \"timestamp_col\"\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.datetime/#get_timedelta_str","title":"<code>get_timedelta_str()</code>","text":"<p>Parameters:</p> Name Type Description <code>timedelta_string</code> <code>str</code> Timedelta string (e.g., \"1h\", \"2d\", \"3w\"). <p>Example:</p> <pre><code>from fsspec_utils.utils.datetime import get_timedelta_str\n\n# Convert to Polars duration string\npolars_duration = get_timedelta_str(\"1h\")\nprint(polars_duration)\n# \"1h\"\n\n# Convert to Pandas timedelta string\npandas_timedelta = get_timedelta_str(\"2d\", to=\"pandas\")\nprint(pandas_timedelta)\n# \"2 days\"\n</code></pre> <p>| <code>to</code> | <code>str</code> | Defaults to 'polars' |</p> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.datetime/#timestamp_from_string","title":"<code>timestamp_from_string()</code>","text":"<p>Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object</p> <p>using only standard Python libraries. Handles strings with or without timezone information (e.g., '2023-01-01T10:00:00+02:00', '2023-01-01', '10:00:00'). Supports timezone offsets like '+HH:MM' or '+HHMM'. For named timezones (e.g., 'Europe/Paris'), requires Python 3.9+ and the 'tzdata' package to be installed.</p> <p>Parameters:</p> Name Type Description <code>timestamp_str</code> <code>str</code> The string representation of the timestamp (ISO 8601 format). <code>tz</code> <code>str</code>, optional Target timezone identifier (e.g., 'UTC', '+02:00', 'Europe/Paris'). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None. <code>naive</code> <code>bool</code>, optional If True, return a naive datetime/time (no timezone info), even if the input string or <code>tz</code> parameter specifies one. Defaults to False. <p>Returns:</p> <ul> <li><code>Union[dt.datetime, dt.date, dt.time]</code>: The parsed datetime, date, or time object.</li> </ul> <p>Example:</p> <pre><code>from fsspec_utils.utils.datetime import timestamp_from_string\n\n# Parse a timestamp string with timezone\ndt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\")\nprint(dt_obj)\n# 2023-01-01 10:00:00+02:00\n\n# Parse a date string\ndate_obj = timestamp_from_string(\"2023-01-01\")\nprint(date_obj)\n# 2023-01-01\n\n# Parse a time string and localize to UTC\ntime_obj = timestamp_from_string(\"15:30:00\", tz=\"UTC\")\nprint(time_obj)\n# 15:30:00+00:00\n\n# Parse a timestamp and return as naive datetime\nnaive_dt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\", naive=True)\nprint(naive_dt_obj)\n# 2023-01-01 10:00:00\n</code></pre> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the timestamp string format is invalid or the timezone is invalid/unsupported.</li> </ul>"},{"location":"api/fsspec_utils.utils.logging/","title":"<code>fsspec_utils.utils.logging</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.logging/#setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure the Loguru logger for fsspec-utils.</p> <p>Removes the default handler and adds a new one targeting stderr with customizable level and format.</p> <p>Parameters:</p> Name Type Description <code>level</code> <code>str</code>, optional Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL). If None, uses FSSPEC_UTILS_LOG_LEVEL environment variable or defaults to \"INFO\". <code>disable</code> <code>bool</code> Whether to disable logging for fsspec-utils package. <code>format_string</code> <code>str</code>, optional Custom format string for log messages. If None, uses a default comprehensive format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul> <p>Example: <pre><code># Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.logging/#get_logger","title":"<code>get_logger()</code>","text":"<p>Get a logger instance for the given name.</p> <p>Parameters:</p> Name Type Description <code>name</code> <code>str</code> Logger name, typically the module name. <p>Returns:</p> <ul> <li><code>Logger</code>: Configured logger instance.</li> </ul> <p>Example: <pre><code>logger = get_logger(__name__)\nlogger.info(\"This is a log message\")\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.misc/","title":"<code>fsspec_utils.utils.misc</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.misc/#run_parallel","title":"<code>run_parallel()</code>","text":"<p>Run a function for a list of parameters in parallel.</p> <p>Provides parallel execution with progress tracking and flexible argument handling.</p> <p>Parameters:</p> Name Type Description <code>func</code> <code>Callable</code> The function to be executed in parallel. <code>*args</code> <code>Any</code> Positional arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <code>n_jobs</code> <code>int</code> The number of CPU cores to use. -1 means all available cores. <code>backend</code> <code>str</code> The backend to use for parallel processing. Options include 'loky', 'threading', 'multiprocessing', and 'sequential'. <code>verbose</code> <code>bool</code> If True, a progress bar will be displayed during execution. <code>**kwargs</code> <code>Any</code> Keyword arguments to pass to <code>func</code>. If an iterable, <code>func</code> will be called for each item. <p>Returns:</p> <ul> <li><code>list</code>: List of function outputs in the same order as inputs.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If no iterable arguments provided or length mismatch.</li> </ul> <p>Examples: <pre><code># Single iterable argument\nrun_parallel(str.upper, [\"hello\", \"world\"])\n\n# Multiple iterables in args and kwargs\ndef add(x, y, offset=0):\n    return x + y + offset\nrun_parallel(add, [1, 2, 3], y=[4, 5, 6], offset=10)\n\n# Fixed and iterable arguments\nrun_parallel(pow, [2, 3, 4], exp=2)\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.misc/#get_partitions_from_path","title":"<code>get_partitions_from_path()</code>","text":"<p>Extract dataset partitions from a file path.</p> <p>Parses file paths to extract partition information based on different partitioning schemes.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file path from which to extract partition information. <code>partitioning</code> <code>str</code> or <code>list[str]</code> or <code>None</code> The partitioning scheme to use. Can be \"hive\" for Hive-style, a string for a single partition column, a list of strings for multiple partition columns, or None for no specific partitioning. <p>Returns:</p> <ul> <li><code>list[tuple[str, str]]</code>: List of tuples containing (column, value) pairs.</li> </ul> <p>Examples: <pre><code># Hive-style partitioning\nget_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n\n# Single partition column\nget_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n\n# Multiple partition columns\nget_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.misc/#path_to_glob","title":"<code>path_to_glob()</code>","text":"<p>Convert a path to a glob pattern for file matching.</p> <p>Intelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.</p> <p>Parameters:</p> Name Type Description <code>path</code> <code>str</code> The file or directory path to convert into a glob pattern. <code>format</code> <code>str</code> or <code>None</code> The desired file format or extension to match (e.g., \"parquet\", \"csv\", \"json\"). If None, the format is inferred from the path. <p>Returns:</p> <ul> <li><code>str</code>: Glob pattern for matching files</li> </ul> <p>Example: <pre><code># Directory to parquet files glob\npath_to_glob(\"data/\", \"parquet\")\n\n# Already a glob pattern\npath_to_glob(\"data/*.csv\")\n\n# Specific file\npath_to_glob(\"data/file.json\")\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.misc/#check_optional_dependency","title":"<code>check_optional_dependency()</code>","text":"<p>Check if an optional dependency is available.</p> <p>Parameters:</p> Name Type Description <code>package_name</code> <code>str</code> The name of the optional package to check for availability. <code>feature_name</code> <code>str</code> A descriptive name of the feature that requires this package. <p>Raises:</p> <ul> <li><code>ImportError</code>: If the package is not available</li> </ul>"},{"location":"api/fsspec_utils.utils.polars/","title":"<code>fsspec_utils.utils.polars</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.polars/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a Polars DataFrame for performance and memory efficiency.</p> <p>This function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to optimize. <code>include</code> <code>list[str]</code> or <code>None</code> Optional list of column names to include in the optimization process. If None, all columns are considered. <code>exclude</code> <code>list[str]</code> or <code>None</code> Optional list of column names to exclude from the optimization process. <code>time_zone</code> <code>str</code> or <code>None</code> Optional time zone string for datetime parsing. <code>shrink_numerics</code> <code>bool</code> If True, numeric columns will be downcasted to smaller data types if possible without losing precision. <code>allow_unsigned</code> <code>bool</code> If True, unsigned integer types will be considered for numeric column optimization. <code>allow_null</code> <code>bool</code> If True, columns containing only null values will be cast to the Null type. <code>strict</code> <code>bool</code> If True, an error will be raised if any column cannot be optimized (e.g., due to type inference issues). <p>Example:</p> <pre><code>import polars as pl\nfrom fsspec_utils.utils.polars import opt_dtype\n\ndf = pl.DataFrame({\n    \"col_int\": [\"1\", \"2\", \"3\"],\n    \"col_float\": [\"1.1\", \"2.2\", \"3.3\"],\n    \"col_bool\": [\"True\", \"False\", \"True\"],\n    \"col_date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n    \"col_str\": [\"a\", \"b\", \"c\"],\n    \"col_null\": [None, None, None]\n})\noptimized_df = opt_dtype(df, shrink_numerics=True)\nprint(optimized_df.schema)\n# Expected output similar to:\n# Schema({\n#     'col_int': Int8,\n#     'col_float': Float32,\n#     'col_bool': Boolean,\n#     'col_date': Date,\n#     'col_str': Utf8,\n#     'col_null': Null\n# })\n</code></pre> <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: DataFrame with optimized data types</li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#unnest_all","title":"<code>unnest_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>seperator</code> <code>str</code> The separator used to flatten nested column names. Defaults to '_'. <code>fields</code> <code>list[str]</code> or <code>None</code> Optional list of specific fields (structs) to unnest. If None, all struct columns will be unnested. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspec_utils.utils.polars import explode_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values\": [[10, 20], [30]]\n})\nexploded_df = explode_all(df)\nprint(exploded_df)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 values \u2502\n# \u2502 --- \u2506 ---    \u2502\n# \u2502 i64 \u2506 i64    \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 10     \u2502\n# \u2502 1   \u2506 20     \u2502\n# \u2502 2   \u2506 30     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>import polars as pl\nfrom fsspec_utils.utils.polars import unnest_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"data\": [\n        {\"a\": 1, \"b\": {\"c\": 3}},\n        {\"a\": 4, \"b\": {\"c\": 6}}\n    ]\n})\nunnested_df = unnest_all(df, seperator='__')\nprint(unnested_df)\n# shape: (2, 3)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 id  \u2506 data__a \u2506 data__b__c \u2502\n# \u2502 --- \u2506 ---  \u2506 ---     \u2502\n# \u2502 i64 \u2506 i64  \u2506 i64     \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1   \u2506 1    \u2506 3       \u2502\n# \u2502 2   \u2506 4    \u2506 6       \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#explode_all","title":"<code>explode_all()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Example:</p> <pre><code>import polars as pl\nfrom fsspec_utils.utils.polars import drop_null_columns\n\ndf = pl.DataFrame({\n    \"col1\": [1, 2, 3],\n    \"col2\": [None, None, None],\n    \"col3\": [\"a\", None, \"c\"]\n})\ndf_cleaned = drop_null_columns(df)\nprint(df_cleaned)\n# shape: (3, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 col1 \u2506 col3  \u2502\n# \u2502 ---  \u2506 ---   \u2502\n# \u2502 i64  \u2506 str   \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1    \u2506 a     \u2502\n# \u2502 2    \u2506 null  \u2502\n# \u2502 3    \u2506 c     \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#with_strftime_columns","title":"<code>with_strftime_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>strftime</code> <code>str</code> The <code>strftime</code> format string (e.g., \"%Y-%m-%d\" for date, \"%H\" for hour). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to use. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names to use for the generated columns. If None, names are derived from the <code>strftime</code> format. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#with_truncated_columns","title":"<code>with_truncated_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>truncate_by</code> <code>str</code> The duration string to truncate by (e.g., \"1h\", \"1d\", \"1mo\"). <code>timestamp_column</code> <code>str</code> The name of the timestamp column to truncate. Defaults to 'auto' (attempts to infer). <code>column_names</code> <code>list[str]</code> or <code>None</code> Optional list of new column names for the truncated columns. If None, names are derived automatically. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#with_datepart_columns","title":"<code>with_datepart_columns()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>timestamp_column</code> <code>str</code> The name of the timestamp column to extract date parts from. Defaults to 'auto' (attempts to infer). <code>year</code> <code>bool</code> If True, extract the year as a new column. <code>month</code> <code>bool</code> If True, extract the month as a new column. <code>week</code> <code>bool</code> If True, extract the week of the year as a new column. <code>yearday</code> <code>bool</code> If True, extract the day of the year as a new column. <code>monthday</code> <code>bool</code> If True, extract the day of the month as a new column. <code>day</code> <code>bool</code> If True, extract the day of the week (1-7, Monday=1) as a new column. <code>weekday</code> <code>bool</code> If True, extract the weekday (0-6, Monday=0) as a new column. <code>hour</code> <code>bool</code> If True, extract the hour as a new column. <code>minute</code> <code>bool</code> If True, extract the minute as a new column. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string to apply to the timestamp column before extracting parts. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#with_row_count","title":"<code>with_row_count()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <code>over</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition the data by before adding row counts. If None, a global row count is added. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#drop_null_columns","title":"<code>drop_null_columns()</code>","text":"<p>Remove columns with all null values from the DataFrame.</p> <p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Parameters:</p> Name Type Description <code>dfs</code> <code>list[polars.DataFrame]</code> A list of Polars DataFrames to unify their schemas. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#cast_relaxed","title":"<code>cast_relaxed()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to cast. <code>schema</code> <code>dict</code> or <code>polars.Schema</code> The target schema to cast the DataFrame to. Can be a dictionary mapping column names to data types or a Polars Schema object. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#delta","title":"<code>delta()</code>","text":"<p>Parameters:</p> Name Type Description <code>df1</code> <code>polars.DataFrame</code> The first Polars DataFrame. <code>df2</code> <code>polars.DataFrame</code> The second Polars DataFrame. <code>subset</code> <code>list[str]</code> or <code>None</code> Optional list of column names to consider when calculating the delta. If None, all columns are used. <code>eager</code> <code>bool</code> If True, the delta calculation is performed eagerly. Defaults to False (lazy). <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.polars/#partition_by","title":"<code>partition_by()</code>","text":"<p>Parameters:</p> Name Type Description <code>df</code> <code>polars.DataFrame</code> The input Polars DataFrame to partition. <code>timestamp_column</code> <code>str</code> or <code>None</code> The name of the timestamp column to use for time-based partitioning. Defaults to None. <code>columns</code> <code>list[str]</code> or <code>None</code> Optional list of column names to partition by. Defaults to None. <code>strftime</code> <code>str</code> or <code>None</code> Optional <code>strftime</code> format string for time-based partitioning. Defaults to None. <code>timedelta</code> <code>str</code> or <code>None</code> Optional timedelta string (e.g., \"1h\", \"1d\") for time-based partitioning. Defaults to None. <code>num_rows</code> <code>int</code> or <code>None</code> Optional number of rows per partition for row-based partitioning. Defaults to None. <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/","title":"<code>fsspec_utils.utils.pyarrow</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.pyarrow/#dominant_timezone_per_column","title":"<code>dominant_timezone_per_column()</code>","text":"<p>For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).</p> <p>If None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to analyze. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import dominant_timezone_per_column\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschema3 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2, schema3]\n\ndominant_tz = dominant_timezone_per_column(schemas)\nprint(dominant_tz)\n# Expected: {'ts': 'UTC'} (or 'Europe/Berlin' depending on logic)\n</code></pre> <p>Returns:</p> <ul> <li><code>dict</code>: {column_name: dominant_timezone}</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#standardize_schema_timezones_by_majority","title":"<code>standardize_schema_timezones_by_majority()</code>","text":"<p>For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> A list of PyArrow schemas to standardize. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones_by_majority\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschemas = [schema1, schema2]\n\nstandardized_schemas = standardize_schema_timezones_by_majority(schemas)\nprint(standardized_schemas[0].field(\"ts\").type)\nprint(standardized_schemas[1].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin] (or UTC, depending on tie-breaking)\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: A new list of schemas with updated timestamp timezones.</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#standardize_schema_timezones","title":"<code>standardize_schema_timezones()</code>","text":"<p>Standardize timezone info for all timestamp columns in a list of PyArrow schemas.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> The list of PyArrow schemas to process. <code>timezone</code> <code>str</code> or <code>None</code> The target timezone to apply to timestamp columns. If None, timezones are removed. If \"auto\", the most frequent timezone across schemas is used. <p>Example:</p> <pre><code>import pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2]\n\n# Remove timezones\nnew_schemas_naive = standardize_schema_timezones(schemas, timezone=None)\nprint(new_schemas_naive[0].field(\"ts\").type)\n# Expected: timestamp[ns]\n\n# Set a specific timezone\nnew_schemas_berlin = standardize_schema_timezones(schemas, timezone=\"Europe/Berlin\")\nprint(new_schemas_berlin[0].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin]\n</code></pre> <p>Returns:</p> <ul> <li><code>list[pyarrow.Schema]</code>: New schemas with standardized timezone info.</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#unify_schemas","title":"<code>unify_schemas()</code>","text":"<p>Unify a list of PyArrow schemas into a single schema.</p> <p>Parameters:</p> Name Type Description <code>schemas</code> <code>list[pyarrow.Schema]</code> List of PyArrow schemas to unify. <code>use_large_dtypes</code> <code>bool</code> If True, keep large types like large_string. <code>timezone</code> <code>str</code> or <code>None</code> If specified, standardize all timestamp columns to this timezone. If \"auto\", use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns. <code>standardize_timezones</code> <code>bool</code> If True, standardize all timestamp columns to the most frequent timezone. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A unified PyArrow schema.</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#cast_schema","title":"<code>cast_schema()</code>","text":"<p>Cast a PyArrow table to a given schema, updating the schema to match the table's columns.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> The PyArrow table to cast. <code>schema</code> <code>pyarrow.Schema</code> The target schema to cast the table to. <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new PyArrow table with the specified schema.</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#convert_large_types_to_normal","title":"<code>convert_large_types_to_normal()</code>","text":"<p>Convert large types in a PyArrow schema to their standard types.</p> <p>Parameters:</p> Name Type Description <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema to convert. <p>Returns:</p> <ul> <li><code>pyarrow.Schema</code>: A new PyArrow schema with large types converted to standard types.</li> </ul>"},{"location":"api/fsspec_utils.utils.pyarrow/#opt_dtype","title":"<code>opt_dtype()</code>","text":"<p>Optimize data types of a PyArrow Table for performance and memory efficiency.</p> <p>Parameters:</p> Name Type Description <code>table</code> <code>pyarrow.Table</code> <code>include</code> <code>list[str]</code>, optional <code>exclude</code> <code>list[str]</code>, optional <code>time_zone</code> <code>str</code>, optional <code>shrink_numerics</code> <code>bool</code> <code>allow_unsigned</code> <code>bool</code> <code>use_large_dtypes</code> <code>bool</code> <code>strict</code> <code>bool</code> <code>allow_null</code> <code>bool</code> If False, columns that only hold null-like values will not be converted to pyarrow.null(). <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: A new table casted to the optimal schema.</li> </ul>"},{"location":"api/fsspec_utils.utils.sql/","title":"<code>fsspec_utils.utils.sql</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.sql/#sql2pyarrow_filter","title":"<code>sql2pyarrow_filter()</code>","text":"<p>Generates a filter expression for PyArrow based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <code>string</code> <code>str</code> The string containing the filter expression. <code>schema</code> <code>pyarrow.Schema</code> The PyArrow schema used to validate the filter expression. <p>Returns:</p> <ul> <li><code>pyarrow.compute.Expression</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspec_utils.utils.sql/#sql2polars_filter","title":"<code>sql2polars_filter()</code>","text":"<p>Generates a filter expression for Polars based on a given string and schema.</p> <p>Parameters:</p> Name Type Description <p>| <code>string</code> | <code>str</code> | The string containing the filter expression. | | <code>schema</code> | <code>polars.Schema</code> | The Polars schema used to validate the filter expression. |</p> <p>Returns:</p> <ul> <li><code>polars.Expr</code>: The generated filter expression.</li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>: If the input string is invalid or contains unsupported operations.</li> </ul>"},{"location":"api/fsspec_utils.utils.sql/#get_table_names","title":"<code>get_table_names()</code>","text":"<p>Parameters:</p> Name Type Description <code>sql_query</code> <code>str</code> The SQL query string to parse. <p>Example:</p> <pre><code>from fsspec_utils.utils.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)\n# Expected: ['my_table']\n\nquery_join = \"SELECT t1.a, t2.b FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id\"\ntables_join = get_table_names(query_join)\nprint(tables_join)\n# Expected: ['table1', 'table2']\n</code></pre> <p>Returns:</p> <ul> <li><code>None</code></li> </ul>"},{"location":"api/fsspec_utils.utils.types/","title":"<code>fsspec_utils.utils.types</code> API Reference","text":""},{"location":"api/fsspec_utils.utils.types/#dict_to_dataframe","title":"<code>dict_to_dataframe()</code>","text":"<p>Convert a dictionary or list of dictionaries to a Polars DataFrame.</p> <p>Handles various input formats: - Single dict with list values -&gt; DataFrame rows - Single dict with scalar values -&gt; Single row DataFrame - List of dicts with scalar values -&gt; Multi-row DataFrame - List of dicts with list values -&gt; DataFrame with list columns</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>dict</code> or <code>list[dict]</code> The input data, either a dictionary or a list of dictionaries. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting DataFrame. <p>Returns:</p> <ul> <li><code>polars.DataFrame</code>: Polars DataFrame containing the converted data.</li> </ul> <p>Examples: <pre><code># Single dict with list values\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndict_to_dataframe(data)\n\n# Single dict with scalar values\ndata = {'a': 1, 'b': 2}\ndict_to_dataframe(data)\n\n# List of dicts with scalar values\ndata = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\ndict_to_dataframe(data)\n</code></pre></p>"},{"location":"api/fsspec_utils.utils.types/#to_pyarrow_table","title":"<code>to_pyarrow_table()</code>","text":"<p>Convert various data formats to PyArrow Table.</p> <p>Handles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.</p> <p>Parameters:</p> Name Type Description <code>data</code> <code>Any</code> Input data to convert. <code>concat</code> <code>bool</code> Whether to concatenate multiple inputs into single table. <code>unique</code> <code>bool</code> If True, duplicate rows will be removed from the resulting Table. <p>Example:</p> <pre><code>import polars as pl\nimport pyarrow as pa\nfrom fsspec_utils.utils.types import to_pyarrow_table\n\n# Convert Polars DataFrame to PyArrow Table\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n\n# Convert list of dicts to PyArrow Table\ndata = [{\"a\": 1, \"b\": 10}, {\"a\": 2, \"b\": 20}]\ntable_from_dict = to_pyarrow_table(data)\nprint(table_from_dict.to_pydf())\n</code></pre> <p>Returns:</p> <ul> <li><code>pyarrow.Table</code>: PyArrow Table containing the converted data.</li> </ul> <p>Example: <pre><code>df = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n</code></pre></p>"}]}